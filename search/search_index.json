{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Metavisitor is a user-friendly and adaptable software to provide biologists, clinical researchers and possibly diagnostic clinicians with the ability to robustly detect and reconstruct viral genomes from complex deep sequence datasets. A set of modular bioinformatic tools and workflows was implemented as the Metavisitor package in the Galaxy framework. Using the graphical Galaxy workflow editor, users with minimal computational skills can use existing Metavisitor workflows or adapt them to suit specific needs by adding or modifying analysis modules. Reference viral database Metavisitor's workflows use a home-made reference viral database vir2 . This database was made using Galaxy-Workflow-Metavisitor__Workflow_for_nucleic_vir2_generation and Galaxy-Workflow-Metavisitor__Workflow_for_proteic_vir2_generation , that can both be found in Metavisitor's Github . How was nucleic vir2 generated? Downloading every viral sequence from NCBI's nuccore database with the following queries (2018/03/21): txid10239[Organism] NOT txid131567[Organism] NOT phage[All Fields] NOT patent[All Fields] NOT chimeric[Title] NOT vector[Title] NOT method[Title] NOT X174[All Fields] AND 301:10000[Sequence length] txid10239[Organism] NOT txid131567[Organism] NOT phage[All Fields] NOT patent[All Fields] NOT chimeric[Title] NOT vector[Title] NOT method[Title] NOT X174[All Fields] AND 10001:1300000[Sequence length] Clustering sequences with 95% identity and shorter than 10 001 bp using vclust. vir2 is available for download in Figshare Quick Start Users who want to use Metavisitor on the Galaxy Mississippi Server , or got already the Metavisitor suite of tools installed in their own Galaxy server, can jump to the next chapter Prepare input data histories . Availability of Metavisitor tools and workflows Metavisitor has been developed at the ARTbio platform . Its tools are primarily available in GitHub . Its workflows are primarily available in the metavisitor repository Metavisitor tools and workflows are also available in the toolshed Metavisitor tools developed by ARTbio in the ARTbio GitHub blast_to_scaffold blastx_to_scaffold blastparser_and_hits blast_unmatched cap3 cherry_pick_fasta cat_multi_datasets fetch_fasta_from_ncbi oases sequence_format_converter small_rna_maps sr_bowtie yac_clipper Tools from other developers are used in the suite metavisitor-2. These tools are available from the main Galaxy toolshed : name=\"bowtie2\" owner=\"devteam\" name=\"data_manager_bowtie2_index_builder\" owner=\"devteam\" name=\"data_manager_fetch_genome_dbkeys_all_fasta\" owner=\"devteam\" name=\"fasta_compute_length\" owner=\"devteam\" name=\"fasta_filter_by_length\" owner=\"devteam\" name=\"fastx_trimmer\" owner=\"devteam\" name=\"ncbi_blast_plus\" owner=\"devteam\" name=\"data_manager_bowtie_index_builder\" owner=\"iuc\" name=\"khmer_normalize_by_median\" owner=\"iuc\" name=\"sra_tools\" owner=\"iuc\" name=\"trinity\" owner=\"iuc\" name=\"vsearch\" owner=\"iuc\" name=\"regex_find_replace\" owner=\"galaxyp\" name=\"spades\" owner=\"nml\" Availability of Metavisitor tools and workflows for Galaxy instance administrators All metavisitor tools are available from the suite_metavisitor_2 Galaxy Admin can just install this suite of tools by using the Install new tools menu in their Admin panel, searching for \"metavisitor\", and installing the suite_metavisitor_2 tool suite. Galaxy Admins can install the workflows from the metavisitor_workflows repository in the main Galaxy toolshed , which will install in addition all tools needed for Metavisitor. Availability of Metavisitors workflows for any Galaxy instance user. We have deposited the Metavisitors workflows in the myexperiment server , where they are searchable with \"metavisitor\" keyword and can be downloaded and reuploaded to the Galaxy instance. Starting a Metavisitor Galaxy server from scratch In the last section of this documentation, we provide instructions to set up Galaxy server instances from scratch with pre-installed Metavisitor tools and workflows: Based on Ansible: see Metavisitor with GalaxyKickstart (Ansible) Based on Docker: see Metavitor with Docker","title":"Metavisitor"},{"location":"#reference-viral-database","text":"Metavisitor's workflows use a home-made reference viral database vir2 . This database was made using Galaxy-Workflow-Metavisitor__Workflow_for_nucleic_vir2_generation and Galaxy-Workflow-Metavisitor__Workflow_for_proteic_vir2_generation , that can both be found in Metavisitor's Github .","title":"Reference viral database"},{"location":"#how-was-nucleic-vir2-generated","text":"Downloading every viral sequence from NCBI's nuccore database with the following queries (2018/03/21): txid10239[Organism] NOT txid131567[Organism] NOT phage[All Fields] NOT patent[All Fields] NOT chimeric[Title] NOT vector[Title] NOT method[Title] NOT X174[All Fields] AND 301:10000[Sequence length] txid10239[Organism] NOT txid131567[Organism] NOT phage[All Fields] NOT patent[All Fields] NOT chimeric[Title] NOT vector[Title] NOT method[Title] NOT X174[All Fields] AND 10001:1300000[Sequence length] Clustering sequences with 95% identity and shorter than 10 001 bp using vclust. vir2 is available for download in Figshare","title":"How was nucleic vir2 generated?"},{"location":"#quick-start","text":"Users who want to use Metavisitor on the Galaxy Mississippi Server , or got already the Metavisitor suite of tools installed in their own Galaxy server, can jump to the next chapter Prepare input data histories .","title":"Quick Start"},{"location":"#availability-of-metavisitor-tools-and-workflows","text":"Metavisitor has been developed at the ARTbio platform . Its tools are primarily available in GitHub . Its workflows are primarily available in the metavisitor repository Metavisitor tools and workflows are also available in the toolshed","title":"Availability of Metavisitor tools and workflows"},{"location":"#metavisitor-tools-developed-by-artbio-in-the-artbio-github","text":"blast_to_scaffold blastx_to_scaffold blastparser_and_hits blast_unmatched cap3 cherry_pick_fasta cat_multi_datasets fetch_fasta_from_ncbi oases sequence_format_converter small_rna_maps sr_bowtie yac_clipper Tools from other developers are used in the suite metavisitor-2. These tools are available from the main Galaxy toolshed : name=\"bowtie2\" owner=\"devteam\" name=\"data_manager_bowtie2_index_builder\" owner=\"devteam\" name=\"data_manager_fetch_genome_dbkeys_all_fasta\" owner=\"devteam\" name=\"fasta_compute_length\" owner=\"devteam\" name=\"fasta_filter_by_length\" owner=\"devteam\" name=\"fastx_trimmer\" owner=\"devteam\" name=\"ncbi_blast_plus\" owner=\"devteam\" name=\"data_manager_bowtie_index_builder\" owner=\"iuc\" name=\"khmer_normalize_by_median\" owner=\"iuc\" name=\"sra_tools\" owner=\"iuc\" name=\"trinity\" owner=\"iuc\" name=\"vsearch\" owner=\"iuc\" name=\"regex_find_replace\" owner=\"galaxyp\" name=\"spades\" owner=\"nml\"","title":"Metavisitor tools developed by ARTbio in the ARTbio GitHub"},{"location":"#availability-of-metavisitor-tools-and-workflows-for-galaxy-instance-administrators","text":"All metavisitor tools are available from the suite_metavisitor_2 Galaxy Admin can just install this suite of tools by using the Install new tools menu in their Admin panel, searching for \"metavisitor\", and installing the suite_metavisitor_2 tool suite. Galaxy Admins can install the workflows from the metavisitor_workflows repository in the main Galaxy toolshed , which will install in addition all tools needed for Metavisitor.","title":"Availability of Metavisitor tools and workflows for Galaxy instance administrators"},{"location":"#availability-of-metavisitors-workflows-for-any-galaxy-instance-user","text":"We have deposited the Metavisitors workflows in the myexperiment server , where they are searchable with \"metavisitor\" keyword and can be downloaded and reuploaded to the Galaxy instance.","title":"Availability of Metavisitors workflows for any Galaxy instance user."},{"location":"#starting-a-metavisitor-galaxy-server-from-scratch","text":"In the last section of this documentation, we provide instructions to set up Galaxy server instances from scratch with pre-installed Metavisitor tools and workflows: Based on Ansible: see Metavisitor with GalaxyKickstart (Ansible) Based on Docker: see Metavitor with Docker","title":"Starting a Metavisitor Galaxy server from scratch"},{"location":"getting_started/","text":"Getting Started Make sure that you have a recent version of Ansible installed; the playbook has been tested with version 2.1.2.0. Getting the playbook GalaxyKickStart is hosted on github and uses a number of dependent Ansible roles that need to be downloaded as part of the installation step: git clone https://github.com/ARTbio/GalaxyKickStart.git ansible-galaxy install -r requirements_roles.yml -p roles The playbook (here galaxy.yml ) should be in the GalaxyKickStart folder. ls GalaxyKickStart/ CONTRIBUTORS.md docs extra-files galaxy.yml group_vars hosts LICENSE.txt mkdocs.yml pre-commit.sh README.md roles Vagrantfile Deploying galaxy-kickstart on remote machines. Inside the repository you will find a hosts file. This is an example inventory. [artimed] localhost ansible_ssh_user= root ansible_ssh_private_key_file= ~/.ssh/id_rsa ... Here [artimed] is a group, that contains a machine called localhost. The variables defined in group_vars/artimed will be applied to this host. Ansible will connect by ssh to this machine, using the ssh key in ~/.ssh/id_rsa . If you would like to run this playbook on a remote machine by ssh (currently needs to be a debian-type machine), create a new inventory, and change localhost to the IP address of that machine. ansible_ssh_user= user controls under which username to connect to this machine. This user needs to have sudo rights. Then, run the plabook by typing: ansible-playbook --inventory-file inventory_files/ your_inventory_file galaxy.yml You can put multiple machines in your inventory. If you run the playbook a second time, the process will be much faster, since steps that have already been executed will be skipped. Whenever you change a variable (see customizations ), you need to run the playbook again. Deploying galaxy-kickstart on specified clouds Inside the repository you will find a file called inventory_files/cloud . This file serves as an example hosts file for how to deploy galaxy-kickstart on Google Compute Engine(gce), Amazon Web Services(aws), and Jetstream. Please note that the ansible_ssh_user variable in the file changes for each remote target . Specifications for each remote target: Jetstream Image needed to deploy galaxy-kickstart: Ubuntu 14.04.3 Development (jetstream image id: 3c3db94e-377b-4583-83d7-082d1024d74a) Inventory: remote host IP anisble_ssh_user=\"root\" ansible_ssh_private_key_file=\" path/to/your/private/key \" GCE Image needed to deploy galaxy-kickstart: Ubuntu 14.04 LTS Inventory: remote host IP anisble_ssh_user=\"ubuntu\" ansible_ssh_private_key_file=\" path/to/your/private/key \" AWS Image needed to deploy galaxy-kickstart: Ubuntu Server 14.04 LTS (HVM), SSD Volume Type - ami-2d39803a Inventory: target Amazon Web Services IP address ansible_ssh_user=\"ubuntu\" ansible_ssh_private_key_file=\" path/to/your/aws/private/key \"","title":"Getting Started"},{"location":"getting_started/#getting-started","text":"Make sure that you have a recent version of Ansible installed; the playbook has been tested with version 2.1.2.0.","title":"Getting Started"},{"location":"getting_started/#getting-the-playbook","text":"GalaxyKickStart is hosted on github and uses a number of dependent Ansible roles that need to be downloaded as part of the installation step: git clone https://github.com/ARTbio/GalaxyKickStart.git ansible-galaxy install -r requirements_roles.yml -p roles The playbook (here galaxy.yml ) should be in the GalaxyKickStart folder. ls GalaxyKickStart/ CONTRIBUTORS.md docs extra-files galaxy.yml group_vars hosts LICENSE.txt mkdocs.yml pre-commit.sh README.md roles Vagrantfile","title":"Getting the playbook"},{"location":"getting_started/#deploying-galaxy-kickstart-on-remote-machines","text":"Inside the repository you will find a hosts file. This is an example inventory. [artimed] localhost ansible_ssh_user= root ansible_ssh_private_key_file= ~/.ssh/id_rsa ... Here [artimed] is a group, that contains a machine called localhost. The variables defined in group_vars/artimed will be applied to this host. Ansible will connect by ssh to this machine, using the ssh key in ~/.ssh/id_rsa . If you would like to run this playbook on a remote machine by ssh (currently needs to be a debian-type machine), create a new inventory, and change localhost to the IP address of that machine. ansible_ssh_user= user controls under which username to connect to this machine. This user needs to have sudo rights. Then, run the plabook by typing: ansible-playbook --inventory-file inventory_files/ your_inventory_file galaxy.yml You can put multiple machines in your inventory. If you run the playbook a second time, the process will be much faster, since steps that have already been executed will be skipped. Whenever you change a variable (see customizations ), you need to run the playbook again.","title":"Deploying galaxy-kickstart on remote machines."},{"location":"getting_started/#deploying-galaxy-kickstart-on-specified-clouds","text":"Inside the repository you will find a file called inventory_files/cloud . This file serves as an example hosts file for how to deploy galaxy-kickstart on Google Compute Engine(gce), Amazon Web Services(aws), and Jetstream. Please note that the ansible_ssh_user variable in the file changes for each remote target . Specifications for each remote target: Jetstream Image needed to deploy galaxy-kickstart: Ubuntu 14.04.3 Development (jetstream image id: 3c3db94e-377b-4583-83d7-082d1024d74a) Inventory: remote host IP anisble_ssh_user=\"root\" ansible_ssh_private_key_file=\" path/to/your/private/key \" GCE Image needed to deploy galaxy-kickstart: Ubuntu 14.04 LTS Inventory: remote host IP anisble_ssh_user=\"ubuntu\" ansible_ssh_private_key_file=\" path/to/your/private/key \" AWS Image needed to deploy galaxy-kickstart: Ubuntu Server 14.04 LTS (HVM), SSD Volume Type - ami-2d39803a Inventory: target Amazon Web Services IP address ansible_ssh_user=\"ubuntu\" ansible_ssh_private_key_file=\" path/to/your/aws/private/key \"","title":"Deploying galaxy-kickstart on specified clouds"},{"location":"install_metavisitor/","text":"In the next three chapters, we provide documentation on two methods to set up a Galaxy server instances from scratch with pre-installed Metavisitor tools and workflows: Based on GalaxyKickstarter: see Metavisitor with GalaxyKickstarter (Ansible) Based on Docker: see Metavitor with Docker For more detailed information on GalaxyKickStart see its documentation","title":"Intro"},{"location":"metavisitor_access_control/","text":"When you are done with the installation of your own Metavisitor Galaxy instance installation using either GalaxyKickStart or docker , there are a few basic things to know for web access and basic server admin operations 1. Connect web frontpage of your Metavisitor Galaxy We assume that you know the IP address to reach the Metavisitor Galaxy webserver: if you used GalaxyKickStart , you had to indicate this host IP in your hosts inventory file. if you used docker , you had to connect to the host machine with the appropriate IP address. Thus, to access Metavisitor Galaxy webserver, just type this IP address in your web browser. If you did not installed yourself the Metavisitor Galaxy instance, ask the IP address to the person who did it. In case you decided to get the Metavisitor Galaxy served on a subdirectory do not forget to append this /subdirectory in your url which then looks like http:// IP /subdirectory 2. Log to the Galaxy server using the admin credentials: If everything goes well, you should now see the Galaxy Metavisitor home page in your web browser. You have to log as the admin. To do that, go to the User menu and click login This is your first login, thus the admin login is admin@galaxy.org and your admin password is admin However for security, immediately change the admin password. To do this, go again in the Users menu, Preferences And click the Change your password item in the User preferences. Basic admin operation: restart the Metavisitor Galaxy instance As we will see in the next chapter , installations of reference genomes or additional tools in the Galaxy Metavisitor instance imply a Galaxy restart for completion. Here is how to do it. restart Metavisitor Galaxy instance deployed with GalaxyKickStart Connect to the server where the Galaxy instance has been installed either through the ssh connection you have used with GalaxyKickStart in your terminal, type sudo supervisorctl restart galaxy: If everything went fine you should see in your terminal # supervisorctl restart galaxy: galaxy_web: stopped handler0: stopped handler1: stopped handler0: started handler1: started galaxy_web: started That's it, the Galaxy instance has restarted. restart Metavisitor Galaxy instance deployed with docker Connect to the server where the Galaxy instance has been installed either through the ssh connection you have used with GalaxyKickStart connect to you docker host using ssh type docker ps . You should see your Metavisitor docker container running and the name of the container in the NAMES column enter into your container by typing: docker exec -it name_of_the_container bash - in the docker session you can now type sudo supervisorctl restart galaxy: and see also, within the container: # supervisorctl restart galaxy: galaxy_web: stopped handler0: stopped handler1: stopped handler0: started handler1: started galaxy_web: started That's it, the Galaxy instance has restarted. you can leave the container by typing exit","title":"Access and Control Metavisitor Galaxy instance"},{"location":"metavisitor_access_control/#1-connect-web-frontpage-of-your-metavisitor-galaxy","text":"We assume that you know the IP address to reach the Metavisitor Galaxy webserver: if you used GalaxyKickStart , you had to indicate this host IP in your hosts inventory file. if you used docker , you had to connect to the host machine with the appropriate IP address. Thus, to access Metavisitor Galaxy webserver, just type this IP address in your web browser. If you did not installed yourself the Metavisitor Galaxy instance, ask the IP address to the person who did it. In case you decided to get the Metavisitor Galaxy served on a subdirectory do not forget to append this /subdirectory in your url which then looks like http:// IP /subdirectory","title":"1. Connect web frontpage of your Metavisitor Galaxy"},{"location":"metavisitor_access_control/#2-log-to-the-galaxy-server-using-the-admin-credentials","text":"If everything goes well, you should now see the Galaxy Metavisitor home page in your web browser. You have to log as the admin. To do that, go to the User menu and click login This is your first login, thus the admin login is admin@galaxy.org and your admin password is admin However for security, immediately change the admin password. To do this, go again in the Users menu, Preferences And click the Change your password item in the User preferences.","title":"2. Log to the Galaxy server using the admin credentials:"},{"location":"metavisitor_access_control/#basic-admin-operation-restart-the-metavisitor-galaxy-instance","text":"As we will see in the next chapter , installations of reference genomes or additional tools in the Galaxy Metavisitor instance imply a Galaxy restart for completion. Here is how to do it.","title":"Basic admin operation: restart the Metavisitor Galaxy instance"},{"location":"metavisitor_access_control/#restart-metavisitor-galaxy-instance-deployed-with-galaxykickstart","text":"Connect to the server where the Galaxy instance has been installed either through the ssh connection you have used with GalaxyKickStart in your terminal, type sudo supervisorctl restart galaxy: If everything went fine you should see in your terminal # supervisorctl restart galaxy: galaxy_web: stopped handler0: stopped handler1: stopped handler0: started handler1: started galaxy_web: started That's it, the Galaxy instance has restarted.","title":"restart Metavisitor Galaxy instance deployed with GalaxyKickStart"},{"location":"metavisitor_access_control/#restart-metavisitor-galaxy-instance-deployed-with-docker","text":"Connect to the server where the Galaxy instance has been installed either through the ssh connection you have used with GalaxyKickStart connect to you docker host using ssh type docker ps . You should see your Metavisitor docker container running and the name of the container in the NAMES column enter into your container by typing: docker exec -it name_of_the_container bash - in the docker session you can now type sudo supervisorctl restart galaxy: and see also, within the container: # supervisorctl restart galaxy: galaxy_web: stopped handler0: stopped handler1: stopped handler0: started handler1: started galaxy_web: started That's it, the Galaxy instance has restarted. you can leave the container by typing exit","title":"restart Metavisitor Galaxy instance deployed with docker"},{"location":"metavisitor_ansible/","text":"Installing Metavisitor with GalaxyKickStart and Ansible Here, a Deployment Machine will install a Metavisitor Galaxy server on Target Machine . Note that Deployment Machine and Target Machine can both be local or remote machines, and that they can be the same machine. Requirements On the Deployment Machine , git and ansible need to be installed. The Target Machine has to be accessible through ssh connection by the user (you) with root privileges. This implies that a correct ssh private key file is available on your Deployment Machine , for instance ~/.ssh/id_rsa . This key will be used for secure transactions managed by ansible between the Deployment Machine and the Target Machine . see the GalaxyKickStart manual for more detailed informations on how to install appropriate version of Ansible. Getting the ansible playbook This is done on the Deployment Machine by cloning the GalaxyKickStart (GalaxyKickStart) repository hosted by the ARTbio organization : In your terminal, type: git clone https://github.com/ARTbio/GalaxyKickStart.git and navigate in the GalaxyKickStart folder: cd GalaxyKickStart GalaxyKickStart makes use of other Ansible roles -- referenced in the requirements_roles.yml file -- that need to be downloaded as part of the installation step: ansible-galaxy install -r requirements_roles.yml -p roles This command installs additional roles in the roles folder. Adapting the GalaxyKickStart folder to your deployment There are only few things to change in the GalaxyKickStart folder before running ansible. Adapt the ansible inventory file In the GalaxyKickStart/inventory_files folder, there is a file called the metavisitor . For deploying Metavisitor, you need to edit this file so that it just contains [metavisitor] ip address ansible_ssh_user= root ansible_ssh_private_key_file= path/to/the/ssh/private/key The ip address is the address of the Target Machine . The path/to/the/ssh/private/key is the path on the Deployment Machine to your ssh key, to be recognized by the Target Machine . Thus, a practical exemple of the final content on the inventory file metavisitor is: [metavisitor] 192.54.201.126 ansible_ssh_user= root ansible_ssh_private_key_file= ~/.ssh/id_rsa where 192.54.201.126 is the ip address of the Target machine and ~/.ssh/id_rsa the path to the private ssh key. Note that you can also install locally Metavisitor by letting the metavisitor inventory file as is: [metavisitor] localhost ansible_connection=local Adapt the ansible inventory file to an Amazon Web Service (AWS) virtual machine In this specific case, add in the hosts inventory file: [metavisitor] 192.54.201.126 ansible_ssh_user= ubuntu ansible_ssh_private_key_file= ~/.ssh/aws_private_key.pem [aws] 192.54.201.126 In that case aws_private_key.pem is the private ssh key for interacting with aws instances, and the [aws] section will trigger additional actions for accessing the Metavisitor Galaxy instance in the Amazon cloud. Note that in addition the settings of the security group associated to the AWS instance should be as follows: Type |Protocole| Port Range | Source | #comment __________________________________________________________________________________________ HTTP | TCP | 80 | 0.0.0.0/0 | for Galaxy web access SSH | TCP | 22 | 0.0.0.0/0 | for ssh access to the AWS instance Custom TCP Rule | TCP | 21 | 0.0.0.0/0 | for FTP upload to Galaxy Custom TCP Rule | TCP | 49152 - 65534 | 0.0.0.0/0 | for FTP upload to Galaxy The ports 21 and 49152 - 65534 should be open for FTP uploads to the AWS instance, and port 80 should be open for accessing galaxy. Adapt the group_vars/all file for persisting data, if needed. In cases where your Target machine has volumes where you wish the Galaxy data to be persisted in, you have to edit the GalaxyKickStart/group_vars/all file, to indicate the path to this volume on the Target machine . If you don't understand the previous statement, no worries, just don't do anything and skip this step. For others, find the lines #persistent data galaxy_persistent_directory: /export # for IFB it's /root/mydisk, by default, /export in the GalaxyKickStart/group_vars/all file, and change /export to the path of your persistent volume. Note that if /export is not changed, nothing will happen and the deployed galaxy server and all associated data files will be in the /home/galaxy/galaxy folder of the Target Machine . Deploying Metavisitor Galaxy on the Target Machine You are almost done. Navigate with your terminal to your GalaxyKickStart folder and type the following command to run the ansible playbook for deploying metavisitor Galaxy on the Target Machine : ansible-playbook --inventory-file=hosts galaxy.yml If everything is ok, you may be asked to authorize the access to the Target Machine by typing yes in the terminal, and you will see ansible orchestrating the serveur deployment on the Target Machine in this terminal. When the process is finished, you should be able to access the Target Machine by typing its IP address in your web browser. By default the admin login/password is admin@galaxy.org / admin . You should change the password for safety. Re-deploying Metavisitor Galaxy on the Target Machine If you are experimented in using ansible, you may customize your Metavisitor Galaxy instance deployed with GalaxyKickStart by editing the content of GalaxyKickStart . In that case, when your changes are done, just run again the command ansible-playbook --inventory-file=hosts galaxy.yml When you run the playbook a second time, the process will be much faster, since steps that have already been executed are skipped. Whenever you change a variable (see customizations ), you need to run the playbook again. You can put multiple machines in your inventory: a simple way to do this is just copying the line the required number of times with the appropriate ip addresses: [metavisitor] 192.54.201.126 ansible_ssh_user= root ansible_ssh_private_key_file= ~/.ssh/id_rsa 192.54.201.127 ansible_ssh_user= root ansible_ssh_private_key_file= ~/.ssh/id_rsa 192.54.201.128 ansible_ssh_user= root ansible_ssh_private_key_file= ~/.ssh/id_rsa","title":"GalaxyKickStart"},{"location":"metavisitor_ansible/#installing-metavisitor-with-galaxykickstart-and-ansible","text":"Here, a Deployment Machine will install a Metavisitor Galaxy server on Target Machine . Note that Deployment Machine and Target Machine can both be local or remote machines, and that they can be the same machine.","title":"Installing Metavisitor with GalaxyKickStart and Ansible"},{"location":"metavisitor_ansible/#requirements","text":"On the Deployment Machine , git and ansible need to be installed. The Target Machine has to be accessible through ssh connection by the user (you) with root privileges. This implies that a correct ssh private key file is available on your Deployment Machine , for instance ~/.ssh/id_rsa . This key will be used for secure transactions managed by ansible between the Deployment Machine and the Target Machine . see the GalaxyKickStart manual for more detailed informations on how to install appropriate version of Ansible.","title":"Requirements"},{"location":"metavisitor_ansible/#getting-the-ansible-playbook","text":"This is done on the Deployment Machine by cloning the GalaxyKickStart (GalaxyKickStart) repository hosted by the ARTbio organization : In your terminal, type: git clone https://github.com/ARTbio/GalaxyKickStart.git and navigate in the GalaxyKickStart folder: cd GalaxyKickStart GalaxyKickStart makes use of other Ansible roles -- referenced in the requirements_roles.yml file -- that need to be downloaded as part of the installation step: ansible-galaxy install -r requirements_roles.yml -p roles This command installs additional roles in the roles folder.","title":"Getting the ansible playbook"},{"location":"metavisitor_ansible/#adapting-the-galaxykickstart-folder-to-your-deployment","text":"There are only few things to change in the GalaxyKickStart folder before running ansible.","title":"Adapting the GalaxyKickStart folder to your deployment"},{"location":"metavisitor_ansible/#adapt-the-ansible-inventory-file","text":"In the GalaxyKickStart/inventory_files folder, there is a file called the metavisitor . For deploying Metavisitor, you need to edit this file so that it just contains [metavisitor] ip address ansible_ssh_user= root ansible_ssh_private_key_file= path/to/the/ssh/private/key The ip address is the address of the Target Machine . The path/to/the/ssh/private/key is the path on the Deployment Machine to your ssh key, to be recognized by the Target Machine . Thus, a practical exemple of the final content on the inventory file metavisitor is: [metavisitor] 192.54.201.126 ansible_ssh_user= root ansible_ssh_private_key_file= ~/.ssh/id_rsa where 192.54.201.126 is the ip address of the Target machine and ~/.ssh/id_rsa the path to the private ssh key.","title":"Adapt the ansible inventory file"},{"location":"metavisitor_ansible/#note-that-you-can-also-install-locally-metavisitor-by-letting-the-metavisitor-inventory-file-as-is","text":"[metavisitor] localhost ansible_connection=local","title":"Note that you can also install locally Metavisitor by letting the metavisitor inventory file as is:"},{"location":"metavisitor_ansible/#adapt-the-ansible-inventory-file-to-an-amazon-web-service-aws-virtual-machine","text":"In this specific case, add in the hosts inventory file: [metavisitor] 192.54.201.126 ansible_ssh_user= ubuntu ansible_ssh_private_key_file= ~/.ssh/aws_private_key.pem [aws] 192.54.201.126 In that case aws_private_key.pem is the private ssh key for interacting with aws instances, and the [aws] section will trigger additional actions for accessing the Metavisitor Galaxy instance in the Amazon cloud. Note that in addition the settings of the security group associated to the AWS instance should be as follows: Type |Protocole| Port Range | Source | #comment __________________________________________________________________________________________ HTTP | TCP | 80 | 0.0.0.0/0 | for Galaxy web access SSH | TCP | 22 | 0.0.0.0/0 | for ssh access to the AWS instance Custom TCP Rule | TCP | 21 | 0.0.0.0/0 | for FTP upload to Galaxy Custom TCP Rule | TCP | 49152 - 65534 | 0.0.0.0/0 | for FTP upload to Galaxy The ports 21 and 49152 - 65534 should be open for FTP uploads to the AWS instance, and port 80 should be open for accessing galaxy.","title":"Adapt the ansible inventory file to an Amazon Web Service (AWS) virtual machine"},{"location":"metavisitor_ansible/#adapt-the-group_varsall-file-for-persisting-data-if-needed","text":"In cases where your Target machine has volumes where you wish the Galaxy data to be persisted in, you have to edit the GalaxyKickStart/group_vars/all file, to indicate the path to this volume on the Target machine . If you don't understand the previous statement, no worries, just don't do anything and skip this step. For others, find the lines #persistent data galaxy_persistent_directory: /export # for IFB it's /root/mydisk, by default, /export in the GalaxyKickStart/group_vars/all file, and change /export to the path of your persistent volume. Note that if /export is not changed, nothing will happen and the deployed galaxy server and all associated data files will be in the /home/galaxy/galaxy folder of the Target Machine .","title":"Adapt the group_vars/all file for persisting data, if needed."},{"location":"metavisitor_ansible/#deploying-metavisitor-galaxy-on-the-target-machine","text":"You are almost done. Navigate with your terminal to your GalaxyKickStart folder and type the following command to run the ansible playbook for deploying metavisitor Galaxy on the Target Machine : ansible-playbook --inventory-file=hosts galaxy.yml If everything is ok, you may be asked to authorize the access to the Target Machine by typing yes in the terminal, and you will see ansible orchestrating the serveur deployment on the Target Machine in this terminal. When the process is finished, you should be able to access the Target Machine by typing its IP address in your web browser. By default the admin login/password is admin@galaxy.org / admin . You should change the password for safety.","title":"Deploying Metavisitor Galaxy on the Target Machine"},{"location":"metavisitor_ansible/#re-deploying-metavisitor-galaxy-on-the-target-machine","text":"If you are experimented in using ansible, you may customize your Metavisitor Galaxy instance deployed with GalaxyKickStart by editing the content of GalaxyKickStart . In that case, when your changes are done, just run again the command ansible-playbook --inventory-file=hosts galaxy.yml When you run the playbook a second time, the process will be much faster, since steps that have already been executed are skipped. Whenever you change a variable (see customizations ), you need to run the playbook again. You can put multiple machines in your inventory: a simple way to do this is just copying the line the required number of times with the appropriate ip addresses: [metavisitor] 192.54.201.126 ansible_ssh_user= root ansible_ssh_private_key_file= ~/.ssh/id_rsa 192.54.201.127 ansible_ssh_user= root ansible_ssh_private_key_file= ~/.ssh/id_rsa 192.54.201.128 ansible_ssh_user= root ansible_ssh_private_key_file= ~/.ssh/id_rsa","title":"Re-deploying Metavisitor Galaxy on the Target Machine"},{"location":"metavisitor_configure_references/","text":"Once you know how to access to your Metavisitor Galaxy instance with a web browser and are able to perform basic start/stop/restart operations, there is still some work needed to import and configure reference data (reference genomes and databases) so that they are directly available to all Galaxy users for running tools and workflows. Here we provide the step-by-step description of what we did to prepare our Metavisitor instance before performing the analyses described here . 1. Connect to your Metavisitor Galaxy admin account with your web browser 2. Import reference data in an history \"References\" At first, you need to import and prepare the reference datasets you will need for most of the Metavisitor analyses. As a Galaxy admin you will make latter some of these references directly accessible to the Galaxy tools, and/or accessible to any other users by putting them in a Galaxy public library. a. Preliminary actions Click on the Analyze Data menu rename the Unnamed history to References b. Upload nucleotide vir2 fasta file Click on the button on top of the tool bar (left handside of the Galaxy interface) In the open window, click on the Rule-based tab Make sure \"Upload data as:\" is set to datasets and \"Load tabular data from:\" is set to Pasted Table Copy - paste the following table (not including the header) Name URL nucleotide vir2 https://ndownloader.figshare.com/files/11005121 protein vir2 https://ndownloader.figshare.com/files/11005124 dm6 ftp://ftp.flybase.net/genomes/Drosophila_melanogaster/dmel_r6.10_FB2016_02/fasta/dmel-all-chromosome-r6.10.fasta.gz AgamP4 https://www.vectorbase.org/sites/default/files/ftp/downloads/Anopheles-gambiae-PEST_CHROMOSOMES_AgamP4.fa.gz P. berghei ftp://ftp.ensemblgenomes.org/pub/release-28/protists/fasta/plasmodium_berghei/dna/Plasmodium_berghei.May_2010.28.dna_sm.genome.fa.gz hg38 ftp://ftp.ensembl.org/pub/release-84/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz Click on the Build button on the bottom right Click on the + Rules button on the bottom left Select Add / Modify Column Definitions from the list Click the + Add Definition button, then select Name from the list and set column \"A\" as Name column Click the + Add Definition button, select URL and set the \"B\" column as URL column Click the Apply button. Finally click the Upload button in the bottom right The reference genomes should be uploaded shortly to Galaxy. 3. Prepare Blast databases Use the tool NCBI BLAST+ makeblastdb What to set in each form field for nucleotide vir2 protein vir2 Molecule type of input nucleotide protein Input FASTA files(s) dataset 1 (nucleotide vir2) dataset 2 (protein vir2) Title for BLAST database nucleotide vir2 blastdb protein vir2 blastdb Leave the rest of the form unchanged and click the Execute button Rename the generated datasets \" nucleotide vir2 blast database \" and \" protein vir2 blast database \" for clarity 4. Creating Galaxy dbkey and fasta references accessible to tools for every user Be sure that the References history is selected in the background, otherwise the uploaded genomes will not be available. Go to the admin panel Click Local data in the left menu Select the Create DBKey and Reference Genome in the \" Data Managers \" table What to set in each form field for nucleotide vir2 dm6 AgamP4 hg38 Use existing dbkey or create a new one New New New New dbkey vir2 dm6 AgamP4 hg38 Choose the source for the reference genome History History History History FASTA file nucleotide vir2 dm6 AgamP4 hg38 Leave the rest of the fields empty and click the Execute button Tip: Once you have run the first job. You can expand the new dataset that appeared in your history and click on the button, instead of going back to the admin panel. 5. Creating Galaxy bowtie indexes accessible to tools for every user Now we are going to generate the bowtie indexes using another data manager tool. Go to the admin panel Click Local data in the left menu Select the Bowtie index builder in the \" Data Managers \" table Select \" vir2 \" in the \"Source FASTA Sequence\" Leave the other options empty and click the Execute button Expand the \" bowtie index \" dataset that appeared in your history and click the button Repeat the previous 3 steps for \" dm6 \", \" AgamP4 \" and \" hg38 \" Note that the preparation of bowtie indexes can be long (several hours for the vir2 bowtie index for instance) 6. Creating Galaxy bowtie2 indexes accessible to tools for every user Finally, we are going to generate the bowtie2 indexes using another data manager tool. Go to the admin panel Click Local data in the left menu Select the Bowtie2 index builder in the \" Data Managers \" table Select \" vir2 \" in the \"Source FASTA Sequence\" Leave the other options empty and click the Execute button Expand the \" bowtie index \" dataset that appeared in your history and click the button Repeat the previous 3 steps for \" AgamP4 \" and \" hg38 \" Note that the preparation of bowtie2 indexes can be long too (several hours for the vir2 bowtie2 index for instance)","title":"Prepare Metavisitor Galaxy instance for analyses"},{"location":"metavisitor_configure_references/#1-connect-to-your-metavisitor-galaxy-admin-account-with-your-web-browser","text":"","title":"1. Connect to your Metavisitor Galaxy admin account with your web browser"},{"location":"metavisitor_configure_references/#2-import-reference-data-in-an-history-references","text":"At first, you need to import and prepare the reference datasets you will need for most of the Metavisitor analyses. As a Galaxy admin you will make latter some of these references directly accessible to the Galaxy tools, and/or accessible to any other users by putting them in a Galaxy public library.","title":"2. Import reference data in an history \"References\""},{"location":"metavisitor_configure_references/#a-preliminary-actions","text":"Click on the Analyze Data menu rename the Unnamed history to References","title":"a. Preliminary actions"},{"location":"metavisitor_configure_references/#b-upload-nucleotide-vir2-fasta-file","text":"Click on the button on top of the tool bar (left handside of the Galaxy interface) In the open window, click on the Rule-based tab Make sure \"Upload data as:\" is set to datasets and \"Load tabular data from:\" is set to Pasted Table Copy - paste the following table (not including the header) Name URL nucleotide vir2 https://ndownloader.figshare.com/files/11005121 protein vir2 https://ndownloader.figshare.com/files/11005124 dm6 ftp://ftp.flybase.net/genomes/Drosophila_melanogaster/dmel_r6.10_FB2016_02/fasta/dmel-all-chromosome-r6.10.fasta.gz AgamP4 https://www.vectorbase.org/sites/default/files/ftp/downloads/Anopheles-gambiae-PEST_CHROMOSOMES_AgamP4.fa.gz P. berghei ftp://ftp.ensemblgenomes.org/pub/release-28/protists/fasta/plasmodium_berghei/dna/Plasmodium_berghei.May_2010.28.dna_sm.genome.fa.gz hg38 ftp://ftp.ensembl.org/pub/release-84/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz Click on the Build button on the bottom right Click on the + Rules button on the bottom left Select Add / Modify Column Definitions from the list Click the + Add Definition button, then select Name from the list and set column \"A\" as Name column Click the + Add Definition button, select URL and set the \"B\" column as URL column Click the Apply button. Finally click the Upload button in the bottom right The reference genomes should be uploaded shortly to Galaxy.","title":"b. Upload nucleotide vir2 fasta file"},{"location":"metavisitor_configure_references/#3-prepare-blast-databases","text":"Use the tool NCBI BLAST+ makeblastdb What to set in each form field for nucleotide vir2 protein vir2 Molecule type of input nucleotide protein Input FASTA files(s) dataset 1 (nucleotide vir2) dataset 2 (protein vir2) Title for BLAST database nucleotide vir2 blastdb protein vir2 blastdb Leave the rest of the form unchanged and click the Execute button Rename the generated datasets \" nucleotide vir2 blast database \" and \" protein vir2 blast database \" for clarity","title":"3. Prepare Blast databases"},{"location":"metavisitor_configure_references/#4-creating-galaxy-dbkey-and-fasta-references-accessible-to-tools-for-every-user","text":"Be sure that the References history is selected in the background, otherwise the uploaded genomes will not be available. Go to the admin panel Click Local data in the left menu Select the Create DBKey and Reference Genome in the \" Data Managers \" table What to set in each form field for nucleotide vir2 dm6 AgamP4 hg38 Use existing dbkey or create a new one New New New New dbkey vir2 dm6 AgamP4 hg38 Choose the source for the reference genome History History History History FASTA file nucleotide vir2 dm6 AgamP4 hg38 Leave the rest of the fields empty and click the Execute button Tip: Once you have run the first job. You can expand the new dataset that appeared in your history and click on the button, instead of going back to the admin panel.","title":"4. Creating Galaxy dbkey and fasta references accessible to tools for every user"},{"location":"metavisitor_configure_references/#5-creating-galaxy-bowtie-indexes-accessible-to-tools-for-every-user","text":"Now we are going to generate the bowtie indexes using another data manager tool. Go to the admin panel Click Local data in the left menu Select the Bowtie index builder in the \" Data Managers \" table Select \" vir2 \" in the \"Source FASTA Sequence\" Leave the other options empty and click the Execute button Expand the \" bowtie index \" dataset that appeared in your history and click the button Repeat the previous 3 steps for \" dm6 \", \" AgamP4 \" and \" hg38 \" Note that the preparation of bowtie indexes can be long (several hours for the vir2 bowtie index for instance)","title":"5. Creating Galaxy bowtie indexes accessible to tools for every user"},{"location":"metavisitor_configure_references/#6-creating-galaxy-bowtie2-indexes-accessible-to-tools-for-every-user","text":"Finally, we are going to generate the bowtie2 indexes using another data manager tool. Go to the admin panel Click Local data in the left menu Select the Bowtie2 index builder in the \" Data Managers \" table Select \" vir2 \" in the \"Source FASTA Sequence\" Leave the other options empty and click the Execute button Expand the \" bowtie index \" dataset that appeared in your history and click the button Repeat the previous 3 steps for \" AgamP4 \" and \" hg38 \" Note that the preparation of bowtie2 indexes can be long too (several hours for the vir2 bowtie2 index for instance)","title":"6. Creating Galaxy bowtie2 indexes accessible to tools for every user"},{"location":"metavisitor_docker/","text":"Installing Metavisitor with Docker We distribute a docker image of Metavisitor, which can thus be used to run a Metavisitor docker container. For a quick start, go directly to the last section \"Persisting to disk\". Requirements You need to have docker installed and configured for your user. Running images from the dockerhub You can search for pre-built docker images from the dockerhub by typing in the terminal of the machine where you want to run the docker container: docker search metavisitor Then, to get the docker image, type: docker pull artbio/metavisitor-2 In this documentation, we recommend to use the artbio/metavisitor-2 which better corresponds to the environment described in our Metavisitor preprint When this pull is done (may take a few minutes depending on your connection speed to the dockerhub), you can start the container by typing: docker run -d -p 80:80 artbio/metavisitor-2 This command starts a container in daemon mode ( -d ) from the image and serve it on port 80 of the local machine in the standard docker way. -p 80:80 forwards requests to nginx inside the container running on port 80. If you want to access the machine hosting the running container through another port (for instance 8080), just change -p 80:80 to -p 8080:80 Runtime changes to pre-built docker images If you wish to reach the container on a subdirectory, add -e NGINX_GALAXY_LOCATION=\"/my-subdirectory\" to the docker call. For instance, docker run -d -e NGINX_GALAXY_LOCATION= /my-subdirectory -p 80:80 artbio/metavisitor-2 will get the metavisitor docker container serving at http://127.0.0.1:80/my-subdirectory . We recommend also changing the default admin user as well, so the command becomes: docker run -d -e NGINX_GALAXY_LOCATION= /my-subdirectory -e GALAXY_CONFIG_ADMIN_USERS=admin@artbio.fr -p 80:80 artbio/galaxy-kickstart-base Note that if you do not make this latest change, the admin login for the metavisitor container is by default admin@galaxy.org and the password is admin . Persisting to disk All changes made to a docker container are by default ephemeral; if you remove the container, the changes are gone. To persist data (this includes the postgresql database, galaxy's config files and your user data), mount a Volume into the containers /export folder. Due to the persistance mechanism (we use bind-mounts inside the container), you need to privilege the container. Thus, assuming you would like to mount your local /my/data folder and persist you Galaxy data in this folder, run docker run -d --privileged -v /my/data:/export -p 80:80 artbio/metavisitor-2 This will run through the persistence tags of the galaxy.yml and export the required files to /export (now on your machine's /my/data). From the new location the files are being bind-mounted back into their original location.","title":"Docker"},{"location":"metavisitor_docker/#installing-metavisitor-with-docker","text":"We distribute a docker image of Metavisitor, which can thus be used to run a Metavisitor docker container. For a quick start, go directly to the last section \"Persisting to disk\".","title":"Installing Metavisitor with Docker"},{"location":"metavisitor_docker/#requirements","text":"You need to have docker installed and configured for your user.","title":"Requirements"},{"location":"metavisitor_docker/#running-images-from-the-dockerhub","text":"You can search for pre-built docker images from the dockerhub by typing in the terminal of the machine where you want to run the docker container: docker search metavisitor Then, to get the docker image, type: docker pull artbio/metavisitor-2 In this documentation, we recommend to use the artbio/metavisitor-2 which better corresponds to the environment described in our Metavisitor preprint When this pull is done (may take a few minutes depending on your connection speed to the dockerhub), you can start the container by typing: docker run -d -p 80:80 artbio/metavisitor-2 This command starts a container in daemon mode ( -d ) from the image and serve it on port 80 of the local machine in the standard docker way. -p 80:80 forwards requests to nginx inside the container running on port 80. If you want to access the machine hosting the running container through another port (for instance 8080), just change -p 80:80 to -p 8080:80","title":"Running images from the dockerhub"},{"location":"metavisitor_docker/#runtime-changes-to-pre-built-docker-images","text":"If you wish to reach the container on a subdirectory, add -e NGINX_GALAXY_LOCATION=\"/my-subdirectory\" to the docker call. For instance, docker run -d -e NGINX_GALAXY_LOCATION= /my-subdirectory -p 80:80 artbio/metavisitor-2 will get the metavisitor docker container serving at http://127.0.0.1:80/my-subdirectory . We recommend also changing the default admin user as well, so the command becomes: docker run -d -e NGINX_GALAXY_LOCATION= /my-subdirectory -e GALAXY_CONFIG_ADMIN_USERS=admin@artbio.fr -p 80:80 artbio/galaxy-kickstart-base Note that if you do not make this latest change, the admin login for the metavisitor container is by default admin@galaxy.org and the password is admin .","title":"Runtime changes to pre-built docker images"},{"location":"metavisitor_docker/#persisting-to-disk","text":"All changes made to a docker container are by default ephemeral; if you remove the container, the changes are gone. To persist data (this includes the postgresql database, galaxy's config files and your user data), mount a Volume into the containers /export folder. Due to the persistance mechanism (we use bind-mounts inside the container), you need to privilege the container. Thus, assuming you would like to mount your local /my/data folder and persist you Galaxy data in this folder, run docker run -d --privileged -v /my/data:/export -p 80:80 artbio/metavisitor-2 This will run through the persistence tags of the galaxy.yml and export the required files to /export (now on your machine's /my/data). From the new location the files are being bind-mounted back into their original location.","title":"Persisting to disk"},{"location":"use_case_1/","text":"Histories for Use Cases 1-1, 1-2, 1-3 and 1-4 As you will see, Histories 1-1, 1-2 and 1-3 are generated in the same way, using their corresponding workflows. These workflows are available in your Galaxy top menu. An important thing to remember is that you will always start from the Input data for Use Cases 1-1, 1-2, 1-3 and 1-4 history, run the appropriate workflow, sending the outputs of the workflow in a new history named accordingly. History for Use Case 1-1. 1. As aforementioned, ensure that you are in the Input data for Use Cases 1-1, 1-2, 1-3 and 1-4 history. You can always control this by using the top menu Users -- Saved History and selecting the desired history. If you don't see the History right bar, just click in addition the top menu Analyze Data 2. Select the appropriate workflow Click now on the Workflow top menu Select the workflow \"Metavisitor: Workflow for Use Case 1-1\" and to see the workflow, select the submenu \"Edit\" Now that you see the workflow, you can directly execute it by clicking the top right wheel icon and selecting \"Run\" 3. Select the appropriate parameters before running the workflow You now see a page with all the workflow steps, whose top part looks like: As pointed by the first red arrow, a parameter has to be provided at runtime of the workflow: the ncbi_guide_ID . In this Use Case as in the other 1-2 and 1-3 Use Cases, you will paste in the ncbi_guide_ID field the NC_007919.3_ value. This is the NCBI identifier for the Nora virus genome sequence which will be retrieved from Genbank during the workflow and used as a guide for the final reconstruction of the Nora virus genome sequence that is \"present\" in the analyzed small RNA sequencing datasets. You have to select an Input dataset collection for Step 1 (second red arrow). However, as there is only one dataset collection in the input history (the one we have prepared in the previous chapter ), there is no other option in the menu than \" SRP013822 \". You have to select the viral nucleotide Blast database for Step 2. Here again there is indeed nothing else to select than the \" nucleotide vir2 blast database \", just because there is only one dataset in the input history with the \"blast database\" type. You can review the other steps of the workflow. But there is no other selection to perform before running the workflow. 4. Running the workflow sending the outputs in a new history We are almost ready, but before clicking the \" Run Workflow \" button there is an important thing to do: - Check the \" Send results to a new history \" checkbox as shown Here And edit the field to \"History for Use Case 1-1\" You can now click the \"Run workflow\" button. This trigger the workflow run. After a few seconds (may be take a while for complex workflows), you will see an alert that the workflow is started, and a link to navigate to the newly created history. When the workflow has finished, if you navigate to the created \"History for Use Case 1-1\", you should see: Note that 30 datasets have been hidden by the workflow for clarity. You just have to click on the \"hidden\" link to unhide these datasets Histories for Use Cases 1-2 and 1-3 Histories for Uses Cases 1-2 and 1-3 are produced in almost the same way as History for Use Case 1-1. Do exactly as described for Use Case 1-1 and - Remember to go back to the Input data for Use Cases 1-1, 1-2, 1-3 and 1-4 history and be sure you are going to run the workflow from that history. - Select the appropriate workflow ! - Remember to Check the \" Send results to a new history \" checkbox, rename the new history appropriately before pressing the \" Run workflow \" button History for remapping in Use Cases 1-1,2,3 Before running the workflow for remapping in Use Cases 1-1,2,3, we need to collect datasets generated in the histories for Use Case 1-1, 1-2 and 1-3 and send them in our Input data for Use Cases 1-1, 1-2, 1-3 and 1-4 history. Update the Input data for Use Cases 1-1, 1-2, 1-3 and 1-4 history This is because the purpose of the workflow for Use Case 1-4 is to remap the raw read sequencing datasets to the viral genomes generated in the previous histories as well as to 2 different Nora virus genomes deposited in Genbank (NC_007919.3 and JX220408). Thus, go back to the Input data for Use Cases 1-1, 1-2, 1-3 and 1-4 history and Use the Retrieve FASTA from NCBI Metavisitor tool to retrieve the NC_007919.3 sequence. Use the Regex Find And Replace tool on the Retrieve FASTA from NCBI (Nucleotide) with queryString 'NC_007919.3' dataset as an input, and put .+ as Find Regex parameter and NC_007919.3 as Replacement parameter. This is just to change the header of the FASTA file and make it more readable. Rename the generated dataset NC_007919.3 for clarity. Use the Retrieve FASTA from NCBI Metavisitor tool to retrieve the JX220408 sequence. Use the Regex Find And Replace tool on the Retrieve FASTA from NCBI (Nucleotide) with queryString 'JX220408' dataset as an input, and put .+ as Find Regex parameter and JX220408.1 as Replacement parameter. Rename the generated dataset JX220408.1 for clarity. Click on the top wheel history icon, select Copy Datasets ; select \"History for Use Case 1-1\" as a Source History, click on the last dataset of the history (Nora_MV_NC_007919.3_guided), select \"Input data for Use Cases 1-1, 1-2...\" as Destination History, and click \"Copy History Items\". Repeat the previous operation for History for Use Case 1-2, selecting the last \"Nora_raw_reads_NC_007919.3_guided\" dataset. And Repeat the previous operation for History for Use Case 1-3, selecting the last \"Nora_Median-Norm-reads_NC_007919.3_guided\" dataset. You may have to refresh your Input data for Use Cases 1-1, 1-2, 1-3 and 1-4 history to reveal the three copied datasets The last step is to create a dataset collection with the 5 Nora virus genomes that you have now in your input data history: just click on the checkbox icon at the top of the history bar, select the 5 corresponding data sets (NC_007919.3, JX220408.1, Nora_MV_NC_007919.3_guided, Nora_raw_reads_NC_007919.3_guided and Nora_Median-Norm-reads_NC_007919.3_guided), click on the For all selected... button, select Build Dataset List , name this list \"Nora virus genomes\", and press the Create list button. We are done with the input data history update ! Generate the History for remapping in Use Cases 1-1,2,3 In the workflow top menu of Galaxy, select the Metavisitor: Workflow for remapping in Use Cases 1-1,2,3 workflow and directly select the run option (you may also look at the workflow before by selection the edit option). Specify \"SRP013822\" for the step 1 option Specify \"Nora virus genomes\" for the step 2 option (you see now why we had to create a dataset collection) Click at the bottom the checkbox Send results to a new history Edit the field that shows up by typing in it: \" History for remapping in Use Cases 1-1,2,3 \" Execute the workflow by clicking the Run workflow button. After few seconds, you may follow the link to the new history running ! History for remapping in Use Case 1-4 This is a simple history to generate because basically, it is similar to the History for Use Case 1-1, but with a slightly modified (and simplified workflow). Navigate back again to your \"base\" history Input data for Use Cases 1-1, 1-2, 1-3 and 1-4 Now the sequence of operations to be performed should be more familiar to you: Top menu Workflow Select Metavisitor: Workflow for Use Case 1-4 and the run option Step 1 (Input Dataset Collection), select the SRP013822 option Step 2 (viral nucleotide BLAST database), select nucleotide vir2 blast database (forced option if everything went well - only one blast database is available in this input history) Click at the bottom the checkbox Send results to a new history Edit the field that shows up by typing in it: \" History for Use Case 1-4 \" Execute the workflow by clicking the Run workflow button. After few seconds, you may follow the link to the new \" History for Use Case 1-4 \" running !","title":"Use Cases 1-1 to 1-4"},{"location":"use_case_1/#histories-for-use-cases-1-1-1-2-1-3-and-1-4","text":"As you will see, Histories 1-1, 1-2 and 1-3 are generated in the same way, using their corresponding workflows. These workflows are available in your Galaxy top menu. An important thing to remember is that you will always start from the Input data for Use Cases 1-1, 1-2, 1-3 and 1-4 history, run the appropriate workflow, sending the outputs of the workflow in a new history named accordingly.","title":"Histories for Use Cases 1-1, 1-2, 1-3 and 1-4"},{"location":"use_case_1/#history-for-use-case-1-1","text":"","title":"History for Use Case 1-1."},{"location":"use_case_1/#1-as-aforementioned-ensure-that-you-are-in-the-input-data-for-use-cases-1-1-1-2-1-3-and-1-4-history","text":"You can always control this by using the top menu Users -- Saved History and selecting the desired history. If you don't see the History right bar, just click in addition the top menu Analyze Data","title":"1. As aforementioned, ensure that you are in the Input data for Use Cases 1-1, 1-2, 1-3 and 1-4 history."},{"location":"use_case_1/#2-select-the-appropriate-workflow","text":"Click now on the Workflow top menu Select the workflow \"Metavisitor: Workflow for Use Case 1-1\" and to see the workflow, select the submenu \"Edit\" Now that you see the workflow, you can directly execute it by clicking the top right wheel icon and selecting \"Run\"","title":"2. Select the appropriate workflow"},{"location":"use_case_1/#3-select-the-appropriate-parameters-before-running-the-workflow","text":"You now see a page with all the workflow steps, whose top part looks like: As pointed by the first red arrow, a parameter has to be provided at runtime of the workflow: the ncbi_guide_ID . In this Use Case as in the other 1-2 and 1-3 Use Cases, you will paste in the ncbi_guide_ID field the NC_007919.3_ value. This is the NCBI identifier for the Nora virus genome sequence which will be retrieved from Genbank during the workflow and used as a guide for the final reconstruction of the Nora virus genome sequence that is \"present\" in the analyzed small RNA sequencing datasets. You have to select an Input dataset collection for Step 1 (second red arrow). However, as there is only one dataset collection in the input history (the one we have prepared in the previous chapter ), there is no other option in the menu than \" SRP013822 \". You have to select the viral nucleotide Blast database for Step 2. Here again there is indeed nothing else to select than the \" nucleotide vir2 blast database \", just because there is only one dataset in the input history with the \"blast database\" type. You can review the other steps of the workflow. But there is no other selection to perform before running the workflow.","title":"3. Select the appropriate parameters before running the workflow"},{"location":"use_case_1/#4-running-the-workflow-sending-the-outputs-in-a-new-history","text":"We are almost ready, but before clicking the \" Run Workflow \" button there is an important thing to do: - Check the \" Send results to a new history \" checkbox as shown Here And edit the field to \"History for Use Case 1-1\" You can now click the \"Run workflow\" button. This trigger the workflow run. After a few seconds (may be take a while for complex workflows), you will see an alert that the workflow is started, and a link to navigate to the newly created history. When the workflow has finished, if you navigate to the created \"History for Use Case 1-1\", you should see: Note that 30 datasets have been hidden by the workflow for clarity. You just have to click on the \"hidden\" link to unhide these datasets","title":"4. Running the workflow sending the outputs in a new history"},{"location":"use_case_1/#histories-for-use-cases-1-2-and-1-3","text":"Histories for Uses Cases 1-2 and 1-3 are produced in almost the same way as History for Use Case 1-1. Do exactly as described for Use Case 1-1 and - Remember to go back to the Input data for Use Cases 1-1, 1-2, 1-3 and 1-4 history and be sure you are going to run the workflow from that history. - Select the appropriate workflow ! - Remember to Check the \" Send results to a new history \" checkbox, rename the new history appropriately before pressing the \" Run workflow \" button","title":"Histories for Use Cases 1-2 and 1-3"},{"location":"use_case_1/#history-for-remapping-in-use-cases-1-123","text":"Before running the workflow for remapping in Use Cases 1-1,2,3, we need to collect datasets generated in the histories for Use Case 1-1, 1-2 and 1-3 and send them in our Input data for Use Cases 1-1, 1-2, 1-3 and 1-4 history.","title":"History for remapping in Use Cases 1-1,2,3"},{"location":"use_case_1/#update-the-input-data-for-use-cases-1-1-1-2-1-3-and-1-4-history","text":"This is because the purpose of the workflow for Use Case 1-4 is to remap the raw read sequencing datasets to the viral genomes generated in the previous histories as well as to 2 different Nora virus genomes deposited in Genbank (NC_007919.3 and JX220408). Thus, go back to the Input data for Use Cases 1-1, 1-2, 1-3 and 1-4 history and Use the Retrieve FASTA from NCBI Metavisitor tool to retrieve the NC_007919.3 sequence. Use the Regex Find And Replace tool on the Retrieve FASTA from NCBI (Nucleotide) with queryString 'NC_007919.3' dataset as an input, and put .+ as Find Regex parameter and NC_007919.3 as Replacement parameter. This is just to change the header of the FASTA file and make it more readable. Rename the generated dataset NC_007919.3 for clarity. Use the Retrieve FASTA from NCBI Metavisitor tool to retrieve the JX220408 sequence. Use the Regex Find And Replace tool on the Retrieve FASTA from NCBI (Nucleotide) with queryString 'JX220408' dataset as an input, and put .+ as Find Regex parameter and JX220408.1 as Replacement parameter. Rename the generated dataset JX220408.1 for clarity. Click on the top wheel history icon, select Copy Datasets ; select \"History for Use Case 1-1\" as a Source History, click on the last dataset of the history (Nora_MV_NC_007919.3_guided), select \"Input data for Use Cases 1-1, 1-2...\" as Destination History, and click \"Copy History Items\". Repeat the previous operation for History for Use Case 1-2, selecting the last \"Nora_raw_reads_NC_007919.3_guided\" dataset. And Repeat the previous operation for History for Use Case 1-3, selecting the last \"Nora_Median-Norm-reads_NC_007919.3_guided\" dataset. You may have to refresh your Input data for Use Cases 1-1, 1-2, 1-3 and 1-4 history to reveal the three copied datasets The last step is to create a dataset collection with the 5 Nora virus genomes that you have now in your input data history: just click on the checkbox icon at the top of the history bar, select the 5 corresponding data sets (NC_007919.3, JX220408.1, Nora_MV_NC_007919.3_guided, Nora_raw_reads_NC_007919.3_guided and Nora_Median-Norm-reads_NC_007919.3_guided), click on the For all selected... button, select Build Dataset List , name this list \"Nora virus genomes\", and press the Create list button. We are done with the input data history update !","title":"Update the Input data for Use Cases 1-1, 1-2, 1-3 and 1-4 history"},{"location":"use_case_1/#generate-the-history-for-remapping-in-use-cases-1-123","text":"In the workflow top menu of Galaxy, select the Metavisitor: Workflow for remapping in Use Cases 1-1,2,3 workflow and directly select the run option (you may also look at the workflow before by selection the edit option). Specify \"SRP013822\" for the step 1 option Specify \"Nora virus genomes\" for the step 2 option (you see now why we had to create a dataset collection) Click at the bottom the checkbox Send results to a new history Edit the field that shows up by typing in it: \" History for remapping in Use Cases 1-1,2,3 \" Execute the workflow by clicking the Run workflow button. After few seconds, you may follow the link to the new history running !","title":"Generate the History for remapping in Use Cases 1-1,2,3"},{"location":"use_case_1/#history-for-remapping-in-use-case-1-4","text":"This is a simple history to generate because basically, it is similar to the History for Use Case 1-1, but with a slightly modified (and simplified workflow).","title":"History for remapping in Use Case 1-4"},{"location":"use_case_1/#navigate-back-again-to-your-base-history-input-data-for-use-cases-1-1-1-2-1-3-and-1-4","text":"Now the sequence of operations to be performed should be more familiar to you: Top menu Workflow Select Metavisitor: Workflow for Use Case 1-4 and the run option Step 1 (Input Dataset Collection), select the SRP013822 option Step 2 (viral nucleotide BLAST database), select nucleotide vir2 blast database (forced option if everything went well - only one blast database is available in this input history) Click at the bottom the checkbox Send results to a new history Edit the field that shows up by typing in it: \" History for Use Case 1-4 \" Execute the workflow by clicking the Run workflow button. After few seconds, you may follow the link to the new \" History for Use Case 1-4 \" running !","title":"Navigate back again to your \"base\" history Input data for Use Cases 1-1, 1-2, 1-3 and 1-4"},{"location":"use_case_2/","text":"Histories for Use Cases 2-1, 2-2 Now that are more familiar with manipulations in Galaxy with the Use Cases 1-1 to 1-4, we will describe the other Use Case analyses more concisely. If you experience lack of skills in basic Galaxy operations (tool usage, copy of datasets, etc), do not hesitate to go back and examine the previous chapters step by step. Discovery of novel viruses Here we are going to use Metavisitor workflows to discover new viruses infecting a laboratory colony of Anopheles coluzzii mosquitoes. Workflow for Use Case 2-1: Takes reads from EBI SRA ERP012577 (small RNAs), assembles contigs, blastx them against vir2 , selects contigs hitting Dicistroviridae proteins, re-assembles selected contigs to align them to the Drosophila C virus (DCV) genome and integrates them to its sequence Worflow for Use Case 2-2: Takes reads from EBI SRA ERS977505 (mRNA), assembles contigs, and blastx them against vir2 Input data for Use Cases 2-1 and 2-2 As for the previous Use Case 1, the first step is to collect all input data in an history that we will name Input data for Use Cases 2-1 and 2-2 Create a new history Rename this history Input data for Use Cases 2-1 and 2-2 For the small RNA sequence datasets (ERP012577) in this study, we are going to use another tool to upload to the Galaxy Metavisitor server: the EBI SRA ENA SRA tool which in the \"Get data\" section of the left tool bar. Click on this tool and enter ERP012577 in the search field that shows up in the European Nucleotide Archive web page, and search. Click on the ERP012577 link. In the column \"Submitted files (galaxy)\" of the table, click on the first \"fastq file 1\". This action should send you back to your Galaxy page automatically and you see the fastq dataset loading (yellow dataset in the history bar). Repeat the exact same operation, for the three other \"fastq file 1\". At final you should have uploaded four fastq datasets corresponding to the sequencing runs \"post_infected_rep1.fastq\", \"post_infected_rep2.fastq\", \"post_non-infected_rep1.fastq\" and \"post_non-infected_rep2.fastq\" Select a dataset to make sure its datatype is fastqsanger.gz . If not click on the crayon button of any one of these four datasets and select the Datatypes tab and set it to fastqsanger.gz . Repeat this operation for the other 3 datasets. Create a dataset collection as previously explained (step 9) and name it Small RNA reads ERP012577 For the RNA sequence datasets (ERS977505) that will be used in Use Case 2-2, use again the EBI SRA ENA SRA tool which in the \"Get data\" section of the left tool bar. Click on this tool and enter ERS977505 in the search field that shows up in the European Nucleotide Archive web page, and search. Click on the ERS977505 link (Sample 1 result found). In the column \"Submitted files (galaxy)\" of the table, click on the first \"fastq file 1\". This action should send you back to your Galaxy page automatically and you see the fastq dataset loading (yellow dataset in the history bar). Repeat the exact same operation for the other \"fastq file 1\" and the two other \"fastq file 2\" In the end you should have uploaded four additional fastq datasets corresponding to the sequencing runs \"IP-isoT-1_AGTCAA_L001_R_1.fastq\", \"IP-isoT-1_AGTCAA_L001_R_2.fastq\", \"IP-isoT-2_ATGTCA_L002_R_1.fastq\" and \"IP-isoT-2_ATGTCA_L002_R_2.fastq\" Create a dataset collection as explained in the previous chapter and name it long read RNAseq datasets . Note that we are not handling the files as paired-ends, thus use the Build dataset list command and not the Build list of dataset pairs command. Use the Retrieve FASTA from NCBI , paste phix174[title] in the \"Query to NCBI in entrez format\" field and select Nucleotide for the NCBI database. This will upload 154 fasta sequences from phix174. Use the wheel icon at the top of the history bar to copy nucleotide vir2 blast database , protein vir2 blast database and P. berghei from the history References to the current history Input data for Use Cases 2-1 and 2-2 . If you don't remember well how to copy datasets between histories, you may read again the explanation here (step 3) Your are now ready for generating Uses Cases 2-1 and 2-2 History for Use Case 2-1 Stay in the current history Input data for Use Cases 2-1 and 2-2 ! In the Workflow menu, select the workflow Metavisitor: Workflow for Use Case 2-1 and directly select Run (you may also look at the workflow using the edit option). Be careful at selecting Small RNA reads ERP012577 in step 1 (Input Dataset Collection). Be careful in selecting P. berghei in step 2 . Be careful in step 3 select : Retrieve FASTA from NCBI (Nucleotide) with queryString 'phix174[title]' In step 4, the option protein vir2 blast database is forced, because the workflow is expecting of protein blast database in this step and only one dataset with this datatype is available in the history Click the Send results to a new history checkbox and rename the history to \"History for Use Case 2-1\". Run Workflow ! You may follow the link to the new history when the workflow has started. History for Use Case 2-2 If you are not already in, go back to the history Input data for Use Cases 2-1 and 2-2 In the Workflow menu, select the workflow Metavisitor: Workflow for Use Case 2-2 and directly select Run (you may also look at the workflow using the edit option) Be careful at selecting long read RNAseq datasets in step 1 (Input Dataset Collection) In step 2, the option protein vir2 blast database is forced, because the workflow is expecting of protein blast database in this step and only one dataset with this datatype is available in the history Click the Send results to a new history checkbox and rename the history to \"History for Use Case 2-2\". Run Workflow. Re-mapping of the small RNA reads (ERP012577) to the AnCV genome (KU169878). The previous Workflow for Use Case 2-2 allowed to assemble a large contig of 8919 nt which significantly matched structural and non-structural polyproteins of Drosophila C Virus and Cricket Paralysis Virus in blastx alignments (see the dataset blastx Filter sequences by length on data 17 vs 'protein BLAST database from data 2' of the history). This large contig corresponds to the genome of a new Anopheles C Virus deposited to the NCBI nucleotide database under accession number KU169878 (see the companion Metavisitor article and Carissimo et al ). Here, we are going to perform manually a few steps, before using another workflow in the history 2-2 to remap the ERP012577 small RNA reads to the AnCV genome. Look at the blast analysis, by subjects dataset and copy the name of the 8919 nt contig that aligned to DCV and CrPV sequences. It is noteworthy that the names may vary from one Oases run to another because the Oases algorithm is not totally deterministic. In the companion Metavisitor article , this name was Locus_69_Transcript_1/1_Confidence_0.000_Length_8919. Copy this name, find the tool Pick Fasta sequences with header satisfying a query string in the Galaxy tool bar, and paste the name in the field Select sequences with this string in their header of the tool form. Select the dataset Oases_optimiser on data 21: Denovo assembled transcripts as a source file, and run the tool. Now, we are going to change the header of the previously extracted fasta sequences using the tool Regex Find And Replace . Select the previous dataset Concatenated datasets as input dataset for this tool. Click on + Insert Check . Use .*Confidence(.*)_Length_8919 (or the equivalent you extracted) as Find Regex and Anopheles_C_Virus|KU169878_confidence\\1 as Replacement . Copy the dataset collection Small RNA reads ERP012577 from the history Input data for Use Cases 2-1 and 2-2 into the current history Use Case 2-2 . You may have to refresh the history bar to see this collection and the attached datasets popping up. We are now ready to run the workflow. In the workflow menu, pick up the workflow Metavisitor: Workflow for remapping in Use Cases 2-1,2 and select the run option. In the workflow form, ensure that Small RNA reads ERP012577 are selected in step 1 and Regex Find And Replace on data 28 is selected in step 2 (this should be the case if you followed the instructions). This time, do not check the box Send results to a new history and directly click the Run workflow button. This workflow will provide you with a graphical view of ERP012577 small RNA mapping to the AnCV genome.","title":"Use Cases 2-1 and 2-2"},{"location":"use_case_2/#histories-for-use-cases-2-1-2-2","text":"Now that are more familiar with manipulations in Galaxy with the Use Cases 1-1 to 1-4, we will describe the other Use Case analyses more concisely. If you experience lack of skills in basic Galaxy operations (tool usage, copy of datasets, etc), do not hesitate to go back and examine the previous chapters step by step.","title":"Histories for Use Cases 2-1, 2-2"},{"location":"use_case_2/#discovery-of-novel-viruses","text":"Here we are going to use Metavisitor workflows to discover new viruses infecting a laboratory colony of Anopheles coluzzii mosquitoes. Workflow for Use Case 2-1: Takes reads from EBI SRA ERP012577 (small RNAs), assembles contigs, blastx them against vir2 , selects contigs hitting Dicistroviridae proteins, re-assembles selected contigs to align them to the Drosophila C virus (DCV) genome and integrates them to its sequence Worflow for Use Case 2-2: Takes reads from EBI SRA ERS977505 (mRNA), assembles contigs, and blastx them against vir2","title":"Discovery of novel viruses"},{"location":"use_case_2/#input-data-for-use-cases-2-1-and-2-2","text":"As for the previous Use Case 1, the first step is to collect all input data in an history that we will name Input data for Use Cases 2-1 and 2-2 Create a new history Rename this history Input data for Use Cases 2-1 and 2-2 For the small RNA sequence datasets (ERP012577) in this study, we are going to use another tool to upload to the Galaxy Metavisitor server: the EBI SRA ENA SRA tool which in the \"Get data\" section of the left tool bar. Click on this tool and enter ERP012577 in the search field that shows up in the European Nucleotide Archive web page, and search. Click on the ERP012577 link. In the column \"Submitted files (galaxy)\" of the table, click on the first \"fastq file 1\". This action should send you back to your Galaxy page automatically and you see the fastq dataset loading (yellow dataset in the history bar). Repeat the exact same operation, for the three other \"fastq file 1\". At final you should have uploaded four fastq datasets corresponding to the sequencing runs \"post_infected_rep1.fastq\", \"post_infected_rep2.fastq\", \"post_non-infected_rep1.fastq\" and \"post_non-infected_rep2.fastq\" Select a dataset to make sure its datatype is fastqsanger.gz . If not click on the crayon button of any one of these four datasets and select the Datatypes tab and set it to fastqsanger.gz . Repeat this operation for the other 3 datasets. Create a dataset collection as previously explained (step 9) and name it Small RNA reads ERP012577 For the RNA sequence datasets (ERS977505) that will be used in Use Case 2-2, use again the EBI SRA ENA SRA tool which in the \"Get data\" section of the left tool bar. Click on this tool and enter ERS977505 in the search field that shows up in the European Nucleotide Archive web page, and search. Click on the ERS977505 link (Sample 1 result found). In the column \"Submitted files (galaxy)\" of the table, click on the first \"fastq file 1\". This action should send you back to your Galaxy page automatically and you see the fastq dataset loading (yellow dataset in the history bar). Repeat the exact same operation for the other \"fastq file 1\" and the two other \"fastq file 2\" In the end you should have uploaded four additional fastq datasets corresponding to the sequencing runs \"IP-isoT-1_AGTCAA_L001_R_1.fastq\", \"IP-isoT-1_AGTCAA_L001_R_2.fastq\", \"IP-isoT-2_ATGTCA_L002_R_1.fastq\" and \"IP-isoT-2_ATGTCA_L002_R_2.fastq\" Create a dataset collection as explained in the previous chapter and name it long read RNAseq datasets . Note that we are not handling the files as paired-ends, thus use the Build dataset list command and not the Build list of dataset pairs command. Use the Retrieve FASTA from NCBI , paste phix174[title] in the \"Query to NCBI in entrez format\" field and select Nucleotide for the NCBI database. This will upload 154 fasta sequences from phix174. Use the wheel icon at the top of the history bar to copy nucleotide vir2 blast database , protein vir2 blast database and P. berghei from the history References to the current history Input data for Use Cases 2-1 and 2-2 . If you don't remember well how to copy datasets between histories, you may read again the explanation here (step 3) Your are now ready for generating Uses Cases 2-1 and 2-2","title":"Input data for Use Cases 2-1 and 2-2"},{"location":"use_case_2/#history-for-use-case-2-1","text":"Stay in the current history Input data for Use Cases 2-1 and 2-2 ! In the Workflow menu, select the workflow Metavisitor: Workflow for Use Case 2-1 and directly select Run (you may also look at the workflow using the edit option). Be careful at selecting Small RNA reads ERP012577 in step 1 (Input Dataset Collection). Be careful in selecting P. berghei in step 2 . Be careful in step 3 select : Retrieve FASTA from NCBI (Nucleotide) with queryString 'phix174[title]' In step 4, the option protein vir2 blast database is forced, because the workflow is expecting of protein blast database in this step and only one dataset with this datatype is available in the history Click the Send results to a new history checkbox and rename the history to \"History for Use Case 2-1\". Run Workflow ! You may follow the link to the new history when the workflow has started.","title":"History for Use Case 2-1"},{"location":"use_case_2/#history-for-use-case-2-2","text":"If you are not already in, go back to the history Input data for Use Cases 2-1 and 2-2 In the Workflow menu, select the workflow Metavisitor: Workflow for Use Case 2-2 and directly select Run (you may also look at the workflow using the edit option) Be careful at selecting long read RNAseq datasets in step 1 (Input Dataset Collection) In step 2, the option protein vir2 blast database is forced, because the workflow is expecting of protein blast database in this step and only one dataset with this datatype is available in the history Click the Send results to a new history checkbox and rename the history to \"History for Use Case 2-2\". Run Workflow.","title":"History for Use Case 2-2"},{"location":"use_case_2/#re-mapping-of-the-small-rna-reads-erp012577-to-the-ancv-genome-ku169878","text":"The previous Workflow for Use Case 2-2 allowed to assemble a large contig of 8919 nt which significantly matched structural and non-structural polyproteins of Drosophila C Virus and Cricket Paralysis Virus in blastx alignments (see the dataset blastx Filter sequences by length on data 17 vs 'protein BLAST database from data 2' of the history). This large contig corresponds to the genome of a new Anopheles C Virus deposited to the NCBI nucleotide database under accession number KU169878 (see the companion Metavisitor article and Carissimo et al ). Here, we are going to perform manually a few steps, before using another workflow in the history 2-2 to remap the ERP012577 small RNA reads to the AnCV genome. Look at the blast analysis, by subjects dataset and copy the name of the 8919 nt contig that aligned to DCV and CrPV sequences. It is noteworthy that the names may vary from one Oases run to another because the Oases algorithm is not totally deterministic. In the companion Metavisitor article , this name was Locus_69_Transcript_1/1_Confidence_0.000_Length_8919. Copy this name, find the tool Pick Fasta sequences with header satisfying a query string in the Galaxy tool bar, and paste the name in the field Select sequences with this string in their header of the tool form. Select the dataset Oases_optimiser on data 21: Denovo assembled transcripts as a source file, and run the tool. Now, we are going to change the header of the previously extracted fasta sequences using the tool Regex Find And Replace . Select the previous dataset Concatenated datasets as input dataset for this tool. Click on + Insert Check . Use .*Confidence(.*)_Length_8919 (or the equivalent you extracted) as Find Regex and Anopheles_C_Virus|KU169878_confidence\\1 as Replacement . Copy the dataset collection Small RNA reads ERP012577 from the history Input data for Use Cases 2-1 and 2-2 into the current history Use Case 2-2 . You may have to refresh the history bar to see this collection and the attached datasets popping up. We are now ready to run the workflow. In the workflow menu, pick up the workflow Metavisitor: Workflow for remapping in Use Cases 2-1,2 and select the run option. In the workflow form, ensure that Small RNA reads ERP012577 are selected in step 1 and Regex Find And Replace on data 28 is selected in step 2 (this should be the case if you followed the instructions). This time, do not check the box Send results to a new history and directly click the Run workflow button. This workflow will provide you with a graphical view of ERP012577 small RNA mapping to the AnCV genome.","title":"Re-mapping of the small RNA reads (ERP012577) to the AnCV genome (KU169878)."},{"location":"use_case_3-1/","text":"Now that you are familiar with manipulations in Galaxy with the Use Cases 1-1 to 1-4 described in detail in the previous chapters, we will describe the other Use Case analyses more concisely. If you experience lack of skills in basic Galaxy operations (tool usage, copy of datasets, etc), do not hesitate to go back and examine the previous chapters step by step. Virus detection in human RNAseq libraries In the Use Cases 3-X we'll use Metavisitor to detect viruses in RNA sequencing dataset of human patients from 3 different studies. In Use Case 3-1 we use Metavisitor to detect and assemble HIV genomes from patients Innate lymphoid cells sequencing data EBI SRP068722. Input data for Use Case 3-1 As for the previous Use Cases 1 and 2, the first step is to collect all the input data in a history that we will name Input data for Use Case 3-1 . Create a new history Rename this history Input data for Use Case 3-1 We are going to upload 40 datasets form the EBI ENA SRP068722 : Go to the upload files menu and select Paste/Fetch data . Copy-Paste the following table (excluding the headers): SRR id Patient id SRR3111582 patient 0450-318 SRR3111583 patient 0450-318 SRR3111584 patient 0450-318 SRR3111585 patient 0450-318 SRR3111586 patient 0450-318 SRR3111587 patient 0450-318 SRR3111588 patient 0387-272 SRR3111589 patient 0387-272 SRR3111590 patient 0387-272 SRR3111591 patient 0387-272 SRR3111592 patient 0387-272 SRR3111593 patient 0387-272 SRR3111594 patient 0629-453 SRR3111595 patient 0629-453 SRR3111596 patient 0629-453 SRR3111597 patient 0629-453 SRR3111598 patient 0629-453 SRR3111599 patient 0629-453 SRR3111600 patient 0444-312 SRR3111601 patient 0444-312 SRR3111602 patient 0444-312 SRR3111603 patient 0444-312 SRR3111604 patient 0500-355neg SRR3111605 patient 0500-355neg SRR3111606 patient 0292-xxxneg SRR3111607 patient 0292-xxxneg SRR3111608 patient 0394-274 SRR3111609 patient 0394-274 SRR3111610 patient 0218-162neg SRR3111611 patient 0218-162neg SRR3111612 patient 0311-217HIVneg SRR3111613 patient 0311-217HIVneg SRR3111614 patient 0440-307neg SRR3111616 patient 0440-307neg SRR3111617 patient 0518-370neg SRR3111618 patient 0518-370neg SRR3111619 patient 0560-420neg SRR3111620 patient 0560-420neg SRR3111621 patient 0575-419neg SRR3111622 patient 0575-419neg Click the Start button Name and rename the dataset \"Use-Case_3-1_information\". Use the tool Cut columns from table . In the \"Cut columns field\" write c1 and make sure you select \"Use-Case_3-1_information\" file in the \"From\" field before executing. Rename the output \"Use-Case_3-1_accessions\". Use the tool Extract reads in FASTQ/A format from NCBI SRA , select List of SRA accession, one per line from select input type and \"Use-Case_3-1_accessions\" in sra accession list. Click the Execute button. When the tool is finished running you should have 2 new dataset collections in your history, one of them is empty. Delete the empty collection and verify that you have 40 pairs of datasets in the second collection. If you are missing some sequences you'll have to re-do the steps above with only the missing identifiers. Once done, merge the collections using the tool Merge Collections . Use the Concatenate multiple datasets tail-to-head tool and select \"Paired collection\" as type of data. Set the paired collection as input and select \"Concatenate pairs of datasets\" as type of concatenation. Execute the tool. Rename the outputed collection to SRP068722 and delete the previous one by clicking the X button and selecting \"Permanently Delete Datasets\". Copy the vir2 nucleotide BLAST database from the References history to the current history Input data for Use Case 3-1 . Now we still have to associate sequencing dataset coming from the same patient. We are going to use the tool Tag elements from file to add the patient information as metadata. Click on the Tag elements from file tool and select the collection \"SRP068722\" in \"Input Collection\" and \"Use-Case_3-1_information\" in \"Tag collection elements according to this file\". Execute the tool. Rename the new dataset collection SRP068722_with_patient_information . Select the Apply Rule to Collection and set \"SRP068722_with_patient_information\" as \"Input Collection\". Click on the \"Edit\" button at the right of the form. Click the \"Column\" button and select Add Column from Metadata from the list. In the \"From\" list select \"Tags\". Then click the \"Apply\" button. Click the \"Rules\" button and select Add / Modify Colmn Definitions from the list. Click the \"Add Definition\" button and select the List identifier(s) from the list. In the \"Select a column\" list select \"B\" then click on ... Assign Another Column and select \"A\". Click the \"Apply\" button. Click the \"Save\" and execute the tool. Select the Concatenate multiple datasets tail-to head tool. In \"What type of data do you wish to concatenate?\" select \"Nested collection\". In \"Input nested collection\" select \"SRP068722_with_patient_information (re-organized)\". Execute the tool. Rename the resulting collection \"patient collection\". We are done. You can now permanently delete \"SRP068722_with_patient_information\", \"SRP068722_with_patient_information (re-organized)\" and \"SRP068722\". This will save you some disk space. History for Use Case 3-1 Stay in the history Input data for Use Case 3-1 pick the workflow Metavisitor: Workflow for Use Case 3-1 in the workflows menu, and select the run option. For Step 1 (Fever Patient Sequences collection), select patient collection (this should be already selected). For Step 2, select the nucleotide vir2 blast database (this should also be already selected) As usual, check the box Send results to a new history , edit the name of the new history to History for Use Case 3-1 , and Execute the workflow ! Note, that for complex workflows with dataset collections in input, the actual warning that the workflow is started may take time to show up.","title":"Use Case 3-1"},{"location":"use_case_3-1/#virus-detection-in-human-rnaseq-libraries","text":"In the Use Cases 3-X we'll use Metavisitor to detect viruses in RNA sequencing dataset of human patients from 3 different studies. In Use Case 3-1 we use Metavisitor to detect and assemble HIV genomes from patients Innate lymphoid cells sequencing data EBI SRP068722.","title":"Virus detection in human RNAseq libraries"},{"location":"use_case_3-1/#input-data-for-use-case-3-1","text":"As for the previous Use Cases 1 and 2, the first step is to collect all the input data in a history that we will name Input data for Use Case 3-1 . Create a new history Rename this history Input data for Use Case 3-1 We are going to upload 40 datasets form the EBI ENA SRP068722 : Go to the upload files menu and select Paste/Fetch data . Copy-Paste the following table (excluding the headers): SRR id Patient id SRR3111582 patient 0450-318 SRR3111583 patient 0450-318 SRR3111584 patient 0450-318 SRR3111585 patient 0450-318 SRR3111586 patient 0450-318 SRR3111587 patient 0450-318 SRR3111588 patient 0387-272 SRR3111589 patient 0387-272 SRR3111590 patient 0387-272 SRR3111591 patient 0387-272 SRR3111592 patient 0387-272 SRR3111593 patient 0387-272 SRR3111594 patient 0629-453 SRR3111595 patient 0629-453 SRR3111596 patient 0629-453 SRR3111597 patient 0629-453 SRR3111598 patient 0629-453 SRR3111599 patient 0629-453 SRR3111600 patient 0444-312 SRR3111601 patient 0444-312 SRR3111602 patient 0444-312 SRR3111603 patient 0444-312 SRR3111604 patient 0500-355neg SRR3111605 patient 0500-355neg SRR3111606 patient 0292-xxxneg SRR3111607 patient 0292-xxxneg SRR3111608 patient 0394-274 SRR3111609 patient 0394-274 SRR3111610 patient 0218-162neg SRR3111611 patient 0218-162neg SRR3111612 patient 0311-217HIVneg SRR3111613 patient 0311-217HIVneg SRR3111614 patient 0440-307neg SRR3111616 patient 0440-307neg SRR3111617 patient 0518-370neg SRR3111618 patient 0518-370neg SRR3111619 patient 0560-420neg SRR3111620 patient 0560-420neg SRR3111621 patient 0575-419neg SRR3111622 patient 0575-419neg Click the Start button Name and rename the dataset \"Use-Case_3-1_information\". Use the tool Cut columns from table . In the \"Cut columns field\" write c1 and make sure you select \"Use-Case_3-1_information\" file in the \"From\" field before executing. Rename the output \"Use-Case_3-1_accessions\". Use the tool Extract reads in FASTQ/A format from NCBI SRA , select List of SRA accession, one per line from select input type and \"Use-Case_3-1_accessions\" in sra accession list. Click the Execute button. When the tool is finished running you should have 2 new dataset collections in your history, one of them is empty. Delete the empty collection and verify that you have 40 pairs of datasets in the second collection. If you are missing some sequences you'll have to re-do the steps above with only the missing identifiers. Once done, merge the collections using the tool Merge Collections . Use the Concatenate multiple datasets tail-to-head tool and select \"Paired collection\" as type of data. Set the paired collection as input and select \"Concatenate pairs of datasets\" as type of concatenation. Execute the tool. Rename the outputed collection to SRP068722 and delete the previous one by clicking the X button and selecting \"Permanently Delete Datasets\". Copy the vir2 nucleotide BLAST database from the References history to the current history Input data for Use Case 3-1 . Now we still have to associate sequencing dataset coming from the same patient. We are going to use the tool Tag elements from file to add the patient information as metadata. Click on the Tag elements from file tool and select the collection \"SRP068722\" in \"Input Collection\" and \"Use-Case_3-1_information\" in \"Tag collection elements according to this file\". Execute the tool. Rename the new dataset collection SRP068722_with_patient_information . Select the Apply Rule to Collection and set \"SRP068722_with_patient_information\" as \"Input Collection\". Click on the \"Edit\" button at the right of the form. Click the \"Column\" button and select Add Column from Metadata from the list. In the \"From\" list select \"Tags\". Then click the \"Apply\" button. Click the \"Rules\" button and select Add / Modify Colmn Definitions from the list. Click the \"Add Definition\" button and select the List identifier(s) from the list. In the \"Select a column\" list select \"B\" then click on ... Assign Another Column and select \"A\". Click the \"Apply\" button. Click the \"Save\" and execute the tool. Select the Concatenate multiple datasets tail-to head tool. In \"What type of data do you wish to concatenate?\" select \"Nested collection\". In \"Input nested collection\" select \"SRP068722_with_patient_information (re-organized)\". Execute the tool. Rename the resulting collection \"patient collection\". We are done. You can now permanently delete \"SRP068722_with_patient_information\", \"SRP068722_with_patient_information (re-organized)\" and \"SRP068722\". This will save you some disk space.","title":"Input data for Use Case 3-1"},{"location":"use_case_3-1/#history-for-use-case-3-1","text":"Stay in the history Input data for Use Case 3-1 pick the workflow Metavisitor: Workflow for Use Case 3-1 in the workflows menu, and select the run option. For Step 1 (Fever Patient Sequences collection), select patient collection (this should be already selected). For Step 2, select the nucleotide vir2 blast database (this should also be already selected) As usual, check the box Send results to a new history , edit the name of the new history to History for Use Case 3-1 , and Execute the workflow ! Note, that for complex workflows with dataset collections in input, the actual warning that the workflow is started may take time to show up.","title":"History for Use Case 3-1"},{"location":"use_case_3-2/","text":"Use Case 3-2's aim In this Use Case, Metavisitor is used to search for the presence of viruses and identify them in RNA sequencing data of serums of children suffering from fevers of unknown origins. To compare Metavisitor's results to Yozwiak et al . Input data for Use Case 3-2 As for the previous Use Cases 1, 2 and 3-1, the first step is to collect all input data in an history that we will name Input data for Use Case 3-2 . Create a new history Rename this history Input data for Use Case 3-2 Using the tool Extract reads in FASTQ/A format from NCBI SRA , we are going to upload 43 paired end datasets. Indeed, these 43 datasets correspond to 86 fastq paired-ended sequence files. In addition, some datasets derive from the same patient; in those cases we will merge those datasets using the tool Concatenate multiple datasets tail-to-head and delete and purge the original datasets. Open the \"Upload datasets\" menu. Click on the Paste/Fetch data button, name the file \"Use-Case_3-2_SRR_information\" and copy-paste the following text: SRR id Patient id SRR453487 patient 566 SRR453437 patient 438 SRR453443 patient 401 SRR453458 patient 401 SRR453430 patient 382 SRR453491 patient 377 SRR453499 patient 375 SRR453484 patient 350 SRR453464 patient 349 SRR453506 patient 345 SRR453417 patient 344 SRR453490 patient 335 SRR453478 patient 331 SRR453465 patient 330 SRR453480 patient 330 SRR453489 patient 329 SRR453505 patient 329 SRR453498 patient 322 SRR453446 patient 321 SRR453427 patient 315 SRR453440 patient 315 SRR453438 patient 282 SRR453450 patient 275 SRR453460 patient 274 SRR453485 patient 270 SRR453448 patient 266 SRR453424 patient 263 SRR453457 patient 263 SRR453510 patient 193 SRR453456 patient 187 SRR453425 patient 186 SRR453469 patient 186 SRR453481 patient 183 SRR453531 patient 180 SRR453474 patient 179 SRR453509 patient 171 SRR453451 patient 168 SRR453495 patient 161 SRR453504 patient 161 SRR453500 patient 159 SRR453493 patient 156 SRR453444 patient 131 SRR453426 patient 78 Click the Start button. Select the Cut columns from a table tool and set the \"Cut columns\" parameter to \"c1\" and select \"Use-Case_3-2_SRR_information\" as input file in the \"From\" list. Execute the tool and rename the new dataset collection \"Use_Case_3-2_accessions\". Select the tool Download and Extract Reads in FASTA/Q format from NCBI SRA and select \"List of SRA accession, one per line\" in \"select input type\" and \"Use_Case_3-2_accessions\" in \"sra accession list\". Execute the tool. The data downloading step might take 40 minutes to 1h. Delete \"Single-end data (fastq-dump)\". Select the Concatenate multiple datasets tail-to-head tool and set \"Paired collection\" in \"What type of data do you wish to concatenate?\" and \"Pair-end data (fastq-dump)\" as \"Input paired collection to concatenate\". In \"What type of concatenation do you wish to perform?\" select \"Concatenate pairs of datasets (outputs an unpaired collection of datasets)\". Execute the tool. Select Tag elements from file tool and set \"Concatenation by pairs\" as \"Input Collection\" and \"Use-Case_3-2_SRR_information\" as \"Tag collection elements according to this file\". Execute the tool. Select Apply Rule to Collection tool and set \"data 1, data 144, and others (Tagged)\" as \"Input Collection\" and click the \"Edit\" button. Click the \"Column\" button and select \"Add Column from Metadata\" from the list. Select \"Tags\" from the \"For\" list and click the \"Apply\" button. Click the \"Rules\" button and select \"Add / Modify Column Definitions\". Click \"Add Definitions\" button and select \"List identifier(s)\" from the list. Select \"B\" from the \"Select a column\" list. Click \" ... Assign Another Column \" and select \"A\" from the \"Select column\" list. Click the \"Apply\" button and the \"Save\" button. Execute the tool. Select the Conatenate multiple datasets tail-to-head tool ans set \"Nested collection\" in \"What type of data do you wish to concatenate?\" and select the \"... (re-organized)\" dataset collection in \"Input nested collection\". Click the \"Execute\" button. Rename the output collection as \"Tractable Patient Datasets\". Copy the vir2 nucleotide BLAST database from the References history to the current history Input data for Use Case 3-2 . History for Use Case 3-2 Stay in the history Input data for Use Case 3-2 pick the workflow Metavisitor: Workflow for Use Case 3-2 in the workflows menu, and select the run option. For Step 1 (Fever Patient Sequences collection), select Tractable Patient Datasets (this should be already selected). For Step 2, select the nucleotide vir2 blast database (this should also be already selected) As usual, check the box Send results to a new history , edit the name of the new history to History for Use Case 3-2 , and Execute the workflow ! Note, that for complex workflows with dataset collections in input, the actual warning that the workflow is started make take time to show up; you can even have a \"504 Gateway Time-out\" warning. This is not a serious issue: just go in your User - Saved history menu, you will see you History for Use Case 3-2 running and you will be able to access it. As a last note, the workflow for Use Case 3-2 may take a long time. Be patient.","title":"Use Case 3-2"},{"location":"use_case_3-2/#use-case-3-2s-aim","text":"In this Use Case, Metavisitor is used to search for the presence of viruses and identify them in RNA sequencing data of serums of children suffering from fevers of unknown origins. To compare Metavisitor's results to Yozwiak et al .","title":"Use Case 3-2's aim"},{"location":"use_case_3-2/#input-data-for-use-case-3-2","text":"As for the previous Use Cases 1, 2 and 3-1, the first step is to collect all input data in an history that we will name Input data for Use Case 3-2 . Create a new history Rename this history Input data for Use Case 3-2 Using the tool Extract reads in FASTQ/A format from NCBI SRA , we are going to upload 43 paired end datasets. Indeed, these 43 datasets correspond to 86 fastq paired-ended sequence files. In addition, some datasets derive from the same patient; in those cases we will merge those datasets using the tool Concatenate multiple datasets tail-to-head and delete and purge the original datasets. Open the \"Upload datasets\" menu. Click on the Paste/Fetch data button, name the file \"Use-Case_3-2_SRR_information\" and copy-paste the following text: SRR id Patient id SRR453487 patient 566 SRR453437 patient 438 SRR453443 patient 401 SRR453458 patient 401 SRR453430 patient 382 SRR453491 patient 377 SRR453499 patient 375 SRR453484 patient 350 SRR453464 patient 349 SRR453506 patient 345 SRR453417 patient 344 SRR453490 patient 335 SRR453478 patient 331 SRR453465 patient 330 SRR453480 patient 330 SRR453489 patient 329 SRR453505 patient 329 SRR453498 patient 322 SRR453446 patient 321 SRR453427 patient 315 SRR453440 patient 315 SRR453438 patient 282 SRR453450 patient 275 SRR453460 patient 274 SRR453485 patient 270 SRR453448 patient 266 SRR453424 patient 263 SRR453457 patient 263 SRR453510 patient 193 SRR453456 patient 187 SRR453425 patient 186 SRR453469 patient 186 SRR453481 patient 183 SRR453531 patient 180 SRR453474 patient 179 SRR453509 patient 171 SRR453451 patient 168 SRR453495 patient 161 SRR453504 patient 161 SRR453500 patient 159 SRR453493 patient 156 SRR453444 patient 131 SRR453426 patient 78 Click the Start button. Select the Cut columns from a table tool and set the \"Cut columns\" parameter to \"c1\" and select \"Use-Case_3-2_SRR_information\" as input file in the \"From\" list. Execute the tool and rename the new dataset collection \"Use_Case_3-2_accessions\". Select the tool Download and Extract Reads in FASTA/Q format from NCBI SRA and select \"List of SRA accession, one per line\" in \"select input type\" and \"Use_Case_3-2_accessions\" in \"sra accession list\". Execute the tool. The data downloading step might take 40 minutes to 1h. Delete \"Single-end data (fastq-dump)\". Select the Concatenate multiple datasets tail-to-head tool and set \"Paired collection\" in \"What type of data do you wish to concatenate?\" and \"Pair-end data (fastq-dump)\" as \"Input paired collection to concatenate\". In \"What type of concatenation do you wish to perform?\" select \"Concatenate pairs of datasets (outputs an unpaired collection of datasets)\". Execute the tool. Select Tag elements from file tool and set \"Concatenation by pairs\" as \"Input Collection\" and \"Use-Case_3-2_SRR_information\" as \"Tag collection elements according to this file\". Execute the tool. Select Apply Rule to Collection tool and set \"data 1, data 144, and others (Tagged)\" as \"Input Collection\" and click the \"Edit\" button. Click the \"Column\" button and select \"Add Column from Metadata\" from the list. Select \"Tags\" from the \"For\" list and click the \"Apply\" button. Click the \"Rules\" button and select \"Add / Modify Column Definitions\". Click \"Add Definitions\" button and select \"List identifier(s)\" from the list. Select \"B\" from the \"Select a column\" list. Click \" ... Assign Another Column \" and select \"A\" from the \"Select column\" list. Click the \"Apply\" button and the \"Save\" button. Execute the tool. Select the Conatenate multiple datasets tail-to-head tool ans set \"Nested collection\" in \"What type of data do you wish to concatenate?\" and select the \"... (re-organized)\" dataset collection in \"Input nested collection\". Click the \"Execute\" button. Rename the output collection as \"Tractable Patient Datasets\". Copy the vir2 nucleotide BLAST database from the References history to the current history Input data for Use Case 3-2 .","title":"Input data for Use Case 3-2"},{"location":"use_case_3-2/#history-for-use-case-3-2","text":"Stay in the history Input data for Use Case 3-2 pick the workflow Metavisitor: Workflow for Use Case 3-2 in the workflows menu, and select the run option. For Step 1 (Fever Patient Sequences collection), select Tractable Patient Datasets (this should be already selected). For Step 2, select the nucleotide vir2 blast database (this should also be already selected) As usual, check the box Send results to a new history , edit the name of the new history to History for Use Case 3-2 , and Execute the workflow ! Note, that for complex workflows with dataset collections in input, the actual warning that the workflow is started make take time to show up; you can even have a \"504 Gateway Time-out\" warning. This is not a serious issue: just go in your User - Saved history menu, you will see you History for Use Case 3-2 running and you will be able to access it. As a last note, the workflow for Use Case 3-2 may take a long time. Be patient.","title":"History for Use Case 3-2"},{"location":"use_case_3-3/","text":"Workflow for Use Case 3-3's aim In ths Use Case, we take the datasets from Matranga et al. , relevant in the context of Lassa and Ebola outbreak and epidemic response, to demonstrate the versatility of Metavisitor as well as its ability to generate high throughput reconstruction of viral genomes. Input data for Use Case 3-3 As for the previous Use Cases, the first step is to collect all input data in an history that we will name Input data for Use Case 3-3 . Create a new history Rename this history Input data for Use Case 3-3 Using the tool Extract reads in FASTA/Q format from NCBI SRA , we are going to upload 63 paired end datasets. For Ebola virus samples: Select the upload File tool and click on the Paste/Fetch data button. Name the file \"Ebola_accessions\" and copy-paste the following text: SRR id SRR1613381 SRR1613377 SRR1613382 SRR1613378 SRR1613383 SRR1613379 SRR1613384 SRR1613380 Click the \"Start\" button. Use the Download and Extract Reads in FASTA/Q format from NCBI SRA tool. Set \"List of SRA accession\" in \"select input type\" and enter \"Ebola_accessions\" as input. Execute the tool. Select Concatenate multiple datasets tail-to-head . Change \"What type of data do you wish to concatenate?\" to \"Paired collection\", set the collection as input and \"Concatenate pairs of datasets\" in \"What type of concatenation do you wish to perform?\". When you are finished, you'll have 8 datasets. Make sure to verify their datatype is fastqsanger or fastqsanger.gz , and create a dataset collection (as explained in the previous chapter) of these 8 datasets that you will name Ebola virus . For Lassa virus samples: Upload, Download and Concatenate the Lassa virus datasets the same way as above, but this time name the file \"Lassa_accessions\" and copy-paste this text: SRR id SRR1595772 SRR1595696 SRR1595665 SRR1595500 SRR1594619 SRR1595943 SRR1595673 SRR1595797 SRR1595763 SRR1595558 SRR1594664 SRR1595909 SRR1594651 SRR1595835 SRR1594698 SRR1613388 SRR1613389 SRR1613390 SRR1613391 SRR1613392 SRR1613393 SRR1613394 SRR1613395 SRR1613396 SRR1613397 SRR1613398 SRR1613399 SRR1595853 SRR1606288 SRR1613412 SRR1613403 SRR1606277 SRR1613386 SRR1613387 SRR1606267 SRR1614275 SRR1610580 SRR1595846 SRR1594606 SRR1606236 SRR1594723 SRR1594671 SRR1613414 SRR1613400 SRR1613401 SRR1613404 SRR1613402 SRR1613405 SRR1613407 SRR1613408 SRR1613409 SRR1613410 SRR1613406 SRR1613411 SRR1613413 When you are finished you'll have 55 datasets. Make sure their datatype is fastqsanger or fastqsanger.gz , and create a dataset collection (as explained in the previous chapter) of these 55 datasets that you will name Lassa virus . Copy the nucleotide vir2 blast database from the References history to the current history Input data for Use Case 3-3 . History for Use Case 3-3 / Ebola virus Stay in the history Input data for Use Case 3-3 Pick the workflow Metavisitor: Workflow for Use Case 3-3 in the workflows menu, and select the run option. Before Step 1, you have to specify some parameters at run time. For Ebola virus, the field reference_virus has to be filled with NC_002549.1 (as a guide for reconstruction of the Ebola virus genome) and the field target_virus has to be filled with Ebola . For Step 1, select Ebola virus . For Step 2, select the nucleotide vir2 blast database (this should also be already selected) As usual, check the box Send results to a new history , edit the name of the new history to Use Case 3-3 Ebola virus , and Execute the workflow ! Note, that for complex workflows with dataset collections in input, the actual warning that the workflow is started make take time to show up; you can even have a \"504 Gateway Time-out\" warning. This is not a serious issue: just go in your User - Saved history menu, you will see your Use Case 3-3 Ebola virus history running and you will be able to access it. The workflow for Use Case 3-3 may take a long time. Be patient. History for Use Case 3-3 / Lassa virus, segment L Stay in the history Input data for Use Case 3-3 Pick the workflow Metavisitor: Workflow for Use Case 3-3 in the workflows menu, and select the run option. Before Step 1, you have to specify some parameters at run time. For Lassa virus, the field reference_virus has to be filled with NC_004297.1 (as a guide for reconstruction of the segment L of the Lassa virus genome) and the field target_virus has to be filled with Lassa . For Step 1, select Lassa virus . For Step 2, select the nucleotide vir2 blast database (this should also be already selected) As usual, check the box Send results to a new history , edit the name of the new history to Use Case 3-3 Lassa virus segment L , and Execute the workflow ! Note, that for complex workflows with dataset collections in input, the actual warning that the workflow is started make take time to show up; you can even have a \"504 Gateway Time-out\" warning. This is not a serious issue: just go in your User - Saved history menu, you will see your Use Case 3-3 Lassa virus segment L history running and you will be able to access it. The workflow for Use Case 3-3 may take a long time. Be patient. History for Use Case 3-3 / Lassa virus, segment S Stay in the history Input data for Use Case 3-3 Pick the workflow Metavisitor: Workflow for Use Case 3-3 in the workflows menu, and select the run option. Before Step 1, you have to specify some parameters at run time. For Lassa virus, the field reference_virus has to be filled with NC_004296.1 (as a guide for reconstruction of the segment S of the Lassa virus genome) and the field target_virus has to be filled with Lassa . For Step 1, select Lassa virus (this should be already selected). For Step 2, select the nucleotide vir2 blast database (this should also be already selected) As usual, check the box Send results to a new history , edit the name of the new history to Use Case 3-3 Lassa virus segment S , and Execute the workflow ! Note, that for complex workflows with dataset collections in input, the actual warning that the workflow is started make take time to show up; you can even have a \"504 Gateway Time-out\" warning. This is not a serious issue: just go in your User - Saved history menu, you will see your Use Case 3-3 Lassa virus segment S history running and you will be able to access it. The workflow for Use Case 3-3 may take a long time. Be patient.","title":"Use Case 3-3"},{"location":"use_case_3-3/#workflow-for-use-case-3-3s-aim","text":"In ths Use Case, we take the datasets from Matranga et al. , relevant in the context of Lassa and Ebola outbreak and epidemic response, to demonstrate the versatility of Metavisitor as well as its ability to generate high throughput reconstruction of viral genomes.","title":"Workflow for Use Case 3-3's aim"},{"location":"use_case_3-3/#input-data-for-use-case-3-3","text":"As for the previous Use Cases, the first step is to collect all input data in an history that we will name Input data for Use Case 3-3 . Create a new history Rename this history Input data for Use Case 3-3 Using the tool Extract reads in FASTA/Q format from NCBI SRA , we are going to upload 63 paired end datasets.","title":"Input data for Use Case 3-3"},{"location":"use_case_3-3/#for-ebola-virus-samples","text":"Select the upload File tool and click on the Paste/Fetch data button. Name the file \"Ebola_accessions\" and copy-paste the following text: SRR id SRR1613381 SRR1613377 SRR1613382 SRR1613378 SRR1613383 SRR1613379 SRR1613384 SRR1613380 Click the \"Start\" button. Use the Download and Extract Reads in FASTA/Q format from NCBI SRA tool. Set \"List of SRA accession\" in \"select input type\" and enter \"Ebola_accessions\" as input. Execute the tool. Select Concatenate multiple datasets tail-to-head . Change \"What type of data do you wish to concatenate?\" to \"Paired collection\", set the collection as input and \"Concatenate pairs of datasets\" in \"What type of concatenation do you wish to perform?\". When you are finished, you'll have 8 datasets. Make sure to verify their datatype is fastqsanger or fastqsanger.gz , and create a dataset collection (as explained in the previous chapter) of these 8 datasets that you will name Ebola virus .","title":"For Ebola virus samples:"},{"location":"use_case_3-3/#for-lassa-virus-samples","text":"Upload, Download and Concatenate the Lassa virus datasets the same way as above, but this time name the file \"Lassa_accessions\" and copy-paste this text: SRR id SRR1595772 SRR1595696 SRR1595665 SRR1595500 SRR1594619 SRR1595943 SRR1595673 SRR1595797 SRR1595763 SRR1595558 SRR1594664 SRR1595909 SRR1594651 SRR1595835 SRR1594698 SRR1613388 SRR1613389 SRR1613390 SRR1613391 SRR1613392 SRR1613393 SRR1613394 SRR1613395 SRR1613396 SRR1613397 SRR1613398 SRR1613399 SRR1595853 SRR1606288 SRR1613412 SRR1613403 SRR1606277 SRR1613386 SRR1613387 SRR1606267 SRR1614275 SRR1610580 SRR1595846 SRR1594606 SRR1606236 SRR1594723 SRR1594671 SRR1613414 SRR1613400 SRR1613401 SRR1613404 SRR1613402 SRR1613405 SRR1613407 SRR1613408 SRR1613409 SRR1613410 SRR1613406 SRR1613411 SRR1613413 When you are finished you'll have 55 datasets. Make sure their datatype is fastqsanger or fastqsanger.gz , and create a dataset collection (as explained in the previous chapter) of these 55 datasets that you will name Lassa virus . Copy the nucleotide vir2 blast database from the References history to the current history Input data for Use Case 3-3 .","title":"For Lassa virus samples:"},{"location":"use_case_3-3/#history-for-use-case-3-3-ebola-virus","text":"Stay in the history Input data for Use Case 3-3 Pick the workflow Metavisitor: Workflow for Use Case 3-3 in the workflows menu, and select the run option. Before Step 1, you have to specify some parameters at run time. For Ebola virus, the field reference_virus has to be filled with NC_002549.1 (as a guide for reconstruction of the Ebola virus genome) and the field target_virus has to be filled with Ebola . For Step 1, select Ebola virus . For Step 2, select the nucleotide vir2 blast database (this should also be already selected) As usual, check the box Send results to a new history , edit the name of the new history to Use Case 3-3 Ebola virus , and Execute the workflow ! Note, that for complex workflows with dataset collections in input, the actual warning that the workflow is started make take time to show up; you can even have a \"504 Gateway Time-out\" warning. This is not a serious issue: just go in your User - Saved history menu, you will see your Use Case 3-3 Ebola virus history running and you will be able to access it. The workflow for Use Case 3-3 may take a long time. Be patient.","title":"History for Use Case 3-3 / Ebola virus"},{"location":"use_case_3-3/#history-for-use-case-3-3-lassa-virus-segment-l","text":"Stay in the history Input data for Use Case 3-3 Pick the workflow Metavisitor: Workflow for Use Case 3-3 in the workflows menu, and select the run option. Before Step 1, you have to specify some parameters at run time. For Lassa virus, the field reference_virus has to be filled with NC_004297.1 (as a guide for reconstruction of the segment L of the Lassa virus genome) and the field target_virus has to be filled with Lassa . For Step 1, select Lassa virus . For Step 2, select the nucleotide vir2 blast database (this should also be already selected) As usual, check the box Send results to a new history , edit the name of the new history to Use Case 3-3 Lassa virus segment L , and Execute the workflow ! Note, that for complex workflows with dataset collections in input, the actual warning that the workflow is started make take time to show up; you can even have a \"504 Gateway Time-out\" warning. This is not a serious issue: just go in your User - Saved history menu, you will see your Use Case 3-3 Lassa virus segment L history running and you will be able to access it. The workflow for Use Case 3-3 may take a long time. Be patient.","title":"History for Use Case 3-3 / Lassa virus, segment L"},{"location":"use_case_3-3/#history-for-use-case-3-3-lassa-virus-segment-s","text":"Stay in the history Input data for Use Case 3-3 Pick the workflow Metavisitor: Workflow for Use Case 3-3 in the workflows menu, and select the run option. Before Step 1, you have to specify some parameters at run time. For Lassa virus, the field reference_virus has to be filled with NC_004296.1 (as a guide for reconstruction of the segment S of the Lassa virus genome) and the field target_virus has to be filled with Lassa . For Step 1, select Lassa virus (this should be already selected). For Step 2, select the nucleotide vir2 blast database (this should also be already selected) As usual, check the box Send results to a new history , edit the name of the new history to Use Case 3-3 Lassa virus segment S , and Execute the workflow ! Note, that for complex workflows with dataset collections in input, the actual warning that the workflow is started make take time to show up; you can even have a \"504 Gateway Time-out\" warning. This is not a serious issue: just go in your User - Saved history menu, you will see your Use Case 3-3 Lassa virus segment S history running and you will be able to access it. The workflow for Use Case 3-3 may take a long time. Be patient.","title":"History for Use Case 3-3 / Lassa virus, segment S"},{"location":"use_cases_input_data/","text":"We are now entering into real analyses using Metavisitor. These analyses as well as their biological context are presented as Use Cases in the metavisitor article . We invite readers of this manual to refer to this article if they need to better understand the biological context of the described procedures. In this section, we are going to create step by step a Galaxy history that contains the input data required to run the workflows for Use Cases 1-1, 1-2, 1-3 and 1-4. Detection of known viruses Using small RNA sequencing libraries SRP013822 (EBI ENA) and Metavisitor workflows, we are going to reconstruct Nora virus genomes. Workflow for Use Case 1-1: Takes the raw reads and collapses them into unique sequences to reconstruct a Nora virus genome referred to as Nora_MV Workflow for Use Case 1-2: Takes raw reads and reconstructs a Nora_raw_reads genome Workflow for Use Case 1-3: Takes raw reads, normalizes the abundances and reconstructs a Nora_Median-Norm-reads genome In order to show Metavisitor's ability to detect multiple known viruses we'll use an other workflow with SRP013822 sequences. Workflow for Use Case 1-4: Takes raw reads, assembles contigs and aligns them against vir2 History with input data for Use Cases 1-1, 1-2, 1-3 and 1-4 Create a new history and rename it \"Input data for Use Cases 1-1, 1-2, 1-3 and 1-4\" Get SRP013822 datasets list Use the the tool Upload File and click on the Paste/Fetch data button Copy - Paste the following text (not including the header): SRR id SRR515090 SRR513993 SRR513992 SRR513990 SRR513989 SRR513981 SRR513901 Edit the file name by clicking the \"New File\" section and writing \"use_case_1_accessions\" or by selecting the Start button and changing the file name. Import SRP013822 datasets Use the tool Extract reads in FASTQ/A format from NCBI SRA and select in the select input type list List of SRA accession, one per line . Select in the sra accession list the use_case_1_accessions file and run the tool. Rename a dataset collection SRP013822 Click on the Single-end data (fastqdump) collection Click on the title \"Single-end data (fastqdump)\" and rename it \"SRP013822\" You can delete the Pair-end data (fastq-dump) collection by clicking the X button and selecting \"Collection Only\". Copy the vir2 blast nucleotide database that we prepared earlier in the Reference history. To do so, click on the little wheel icon in the history top menu (in the history right bar). Select \"Copy Datasets\" In the open page, select \"References\" in the Source History menu, check the \"nucleotide vir2 blast database\" dataset; select \"Input data for Use Case 1_1, ...\"; and click the \"Copy History Items\". If you refresh the history, you will see the \"nucleotide vir2 blast database\" dataset showing up. That is all for the moment. We will latter add datasets in the history Input data for Use Cases 1-1, 1-2, 1-3 and 1-4 . However, these datasets do no exist yet: this will be produced by the Use Cases 1-1, 1-2, 1-3 workflows !","title":"Prepare input data histories"},{"location":"use_cases_input_data/#detection-of-known-viruses","text":"Using small RNA sequencing libraries SRP013822 (EBI ENA) and Metavisitor workflows, we are going to reconstruct Nora virus genomes. Workflow for Use Case 1-1: Takes the raw reads and collapses them into unique sequences to reconstruct a Nora virus genome referred to as Nora_MV Workflow for Use Case 1-2: Takes raw reads and reconstructs a Nora_raw_reads genome Workflow for Use Case 1-3: Takes raw reads, normalizes the abundances and reconstructs a Nora_Median-Norm-reads genome In order to show Metavisitor's ability to detect multiple known viruses we'll use an other workflow with SRP013822 sequences. Workflow for Use Case 1-4: Takes raw reads, assembles contigs and aligns them against vir2","title":"Detection of known viruses"},{"location":"use_cases_input_data/#history-with-input-data-for-use-cases-1-1-1-2-1-3-and-1-4","text":"Create a new history and rename it \"Input data for Use Cases 1-1, 1-2, 1-3 and 1-4\" Get SRP013822 datasets list Use the the tool Upload File and click on the Paste/Fetch data button Copy - Paste the following text (not including the header): SRR id SRR515090 SRR513993 SRR513992 SRR513990 SRR513989 SRR513981 SRR513901 Edit the file name by clicking the \"New File\" section and writing \"use_case_1_accessions\" or by selecting the Start button and changing the file name. Import SRP013822 datasets Use the tool Extract reads in FASTQ/A format from NCBI SRA and select in the select input type list List of SRA accession, one per line . Select in the sra accession list the use_case_1_accessions file and run the tool. Rename a dataset collection SRP013822 Click on the Single-end data (fastqdump) collection Click on the title \"Single-end data (fastqdump)\" and rename it \"SRP013822\" You can delete the Pair-end data (fastq-dump) collection by clicking the X button and selecting \"Collection Only\". Copy the vir2 blast nucleotide database that we prepared earlier in the Reference history. To do so, click on the little wheel icon in the history top menu (in the history right bar). Select \"Copy Datasets\" In the open page, select \"References\" in the Source History menu, check the \"nucleotide vir2 blast database\" dataset; select \"Input data for Use Case 1_1, ...\"; and click the \"Copy History Items\". If you refresh the history, you will see the \"nucleotide vir2 blast database\" dataset showing up. That is all for the moment. We will latter add datasets in the history Input data for Use Cases 1-1, 1-2, 1-3 and 1-4 . However, these datasets do no exist yet: this will be produced by the Use Cases 1-1, 1-2, 1-3 workflows !","title":"History with input data for Use Cases 1-1, 1-2, 1-3 and 1-4"}]}